# -*- coding: utf-8 -*-
"""projetFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1657kUfkl0Nj62eaX4Slr_cVrGBN1c92-
"""

import pandas as pd
import random

import pandas as pd
df = pd.read_csv("tweets_viraux2.csv")
df.info()

import pandas as pd
import re



# Extraire les hashtags
df['hashtags'] = df['text'].apply(lambda x: re.findall(r'#\w+', x))

df_exploded = df.explode('hashtags')

stats_hashtags = df_exploded.groupby('hashtags')['likes'].agg(
    ['count', 'mean', 'median', 'max']
).reset_index().sort_values('count', ascending=False)

# Renommer les colonnes
stats_hashtags.columns = ['Hashtag', 'Nombre de tweets', 'Likes moyens', 'Likes m√©dians', 'Likes max']

print(stats_hashtags)

!pip install streamlit
import streamlit as st

st.title("Statistiques des Hashtags")
st.dataframe(stats_hashtags)

pip install transformers torch pandas scikit-learn

from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Charger le tokenizer et le mod√®le pr√©-entra√Æn√©
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 classes : viral ou non

def tokenize_tweet(tweet, max_len=128):
    return tokenizer.encode_plus(
        tweet,
        add_special_tokens=True,
        max_length=max_len,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )

# Exemple
tweet = "This AI news is amazing! #ArtificialIntelligence"
inputs = tokenize_tweet(tweet)

import re
from sklearn.model_selection import train_test_split # Import train_test_split


def clean_text(text):
    """Supprime les hashtags, les mentions et les caract√®res sp√©ciaux du texte."""
    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Supprime les mentions
    text = re.sub(r'#', '', text)  # Supprime le symbole hashtag
    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)  # Supprime les caract√®res sp√©ciaux
    text = re.sub(r'\s+', ' ', text).strip()  # Supprime les espaces suppl√©mentaires
    return text

df['text_clean'] = df['text'].apply(clean_text)  # Applique la fonction de nettoyage √† la colonne 'text'

# Maintenant, vous pouvez proc√©der au train_test_split :
X_train, X_val, y_train, y_val = train_test_split(
    df['text_clean'], df['viral'], test_size=0.2, random_state=42
)

import torch # import the torch module

torch.cuda.empty_cache()

torch.cuda.empty_cache()

!nvidia-smi  # V√©rifiez que le GPU est utilis√© et la m√©moire libre

import tensorflow as tf
print("GPU disponible:", tf.config.list_physical_devices('GPU'))

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Lance une op√©ration lourde pour tester le GPU
x = torch.randn(10000, 10000).to(device)
y = torch.matmul(x, x)

print("Op√©ration termin√©e sur :", device)

!pip install transformers[torch] # Make sure you install with torch support
# Importations
import pandas as pd
import re
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW



# 1. Chargement et pr√©traitement des donn√©es
df = pd.read_csv("tweets_viraux2.csv")

def clean_text(text):
    """Nettoie le texte des mentions, hashtags et caract√®res sp√©ciaux"""
    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Supprime les mentions
    text = re.sub(r'#', '', text)  # Supprime les hashtags
    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)  # Supprime les caract√®res sp√©ciaux
    return re.sub(r'\s+', ' ', text).strip().lower()  # Espaces et minuscules

df['text_clean'] = df['text'].apply(clean_text)

# 2. Split des donn√©es
X_train, X_val, y_train, y_val = train_test_split(
    df['text_clean'], df['viral'], test_size=0.2, random_state=42
)

# 3. Tokenisation globale (optimis√©e)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(
    X_train.tolist(),
    truncation=True,
    padding='max_length',
    max_length=128,
    return_tensors='pt'
)
val_encodings = tokenizer(
    X_val.tolist(),
    truncation=True,
    padding='max_length',
    max_length=128,
    return_tensors='pt'
)

# 4. Pr√©paration des DataLoaders
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_dataset = TensorDataset(
    train_encodings['input_ids'],
    train_encodings['attention_mask'],
    torch.tensor(y_train.values)
)
val_dataset = TensorDataset(
    val_encodings['input_ids'],
    val_encodings['attention_mask'],
    torch.tensor(y_val.values)
)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

# 5. Initialisation du mod√®le
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2
).to(device)
optimizer = AdamW(model.parameters(), lr=5e-5) # Use AdamW from transformers.optimization

# ==== NOUVEAU CODE √Ä COPIER ====
from torch.utils.data import WeightedRandomSampler
from torch.nn import CrossEntropyLoss

# A. Calcul des poids pour r√©√©quilibrer les classes
class_counts = df['viral'].value_counts().to_list()
weights = torch.tensor([1.0/class_counts[0], 1.0/class_counts[1]], device=device)

# B. Sampler pond√©r√©
sampler = WeightedRandomSampler(
    weights=[weights[lab] for lab in y_train],
    num_samples=len(y_train),
    replacement=True
)

# C. DataLoader avec sampler
train_loader = DataLoader(
    train_dataset,
    batch_size=16,
    sampler=sampler,  # Remplace shuffle=True
    num_workers=2
)

# D. Loss et optimizer
criterion = CrossEntropyLoss(weight=weights)
optimizer = AdamW(model.parameters(), lr=2e-5)

# E. Boucle d'entra√Ænement am√©lior√©e
for epoch in range(5):  # 5 epochs maintenant
    model.train()
    total_loss = 0

    for batch in train_loader:
        inputs, masks, labels = [x.to(device) for x in batch]

        optimizer.zero_grad()
        outputs = model(inputs, attention_mask=masks, labels=labels)
        loss = criterion(outputs.logits, labels)  # Loss pond√©r√©e
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # √âvaluation
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in val_loader:
            inputs, masks, labels = [x.to(device) for x in batch]
            outputs = model(inputs, attention_mask=masks)
            preds = torch.argmax(outputs.logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Affichage des m√©triques
    print(f"\nEpoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}")
    print(classification_report(
        all_labels,
        all_preds,
        target_names=['Non Viral', 'Viral'],
        zero_division=0
    ))

# Sauvegarde
torch.save(model.state_dict(), 'bert_balanced.pth')
# ==== FIN DU CODE √Ä COPIER ====

# Importations suppl√©mentaires n√©cessaires
import torch.nn as nn
from torch.nn import functional as F
from imblearn.over_sampling import RandomOverSampler

# 1. R√©√©quilibrage des classes
print("Distribution originale :")
print(df['viral'].value_counts())

ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(
    df[['text_clean']],
    df['viral']
)

# 2. Nouveau split
X_train, X_val, y_train, y_val = train_test_split(
    X_resampled['text_clean'],
    y_resampled,
    test_size=0.2,
    random_state=42
)

# 3. R√©initialisation du mod√®le
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2
).to(device)

# 4. Impl√©mentation corrig√©e de FocalLoss
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return loss.mean()

# 5. Initialisation des composants d'entra√Ænement
criterion = FocalLoss()
optimizer = AdamW(model.parameters(), lr=2e-5)

# 6. Tokenisation des donn√©es r√©√©quilibr√©es
train_encodings = tokenizer(
    X_train.tolist(),
    truncation=True,
    padding='max_length',
    max_length=128,
    return_tensors='pt'
)
val_encodings = tokenizer(
    X_val.tolist(),
    truncation=True,
    padding='max_length',
    max_length=128,
    return_tensors='pt'
)

# 7. Pr√©paration des DataLoaders
train_dataset = TensorDataset(
    train_encodings['input_ids'],
    train_encodings['attention_mask'],
    torch.tensor(y_train.values)
)
val_dataset = TensorDataset(
    val_encodings['input_ids'],
    val_encodings['attention_mask'],
    torch.tensor(y_val.values)
)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

# 8. Boucle d'entra√Ænement
for epoch in range(5):
    model.train()
    total_loss = 0

    for batch in train_loader:
        inputs, masks, labels = [x.to(device) for x in batch]

        optimizer.zero_grad()
        outputs = model(inputs, attention_mask=masks)
        loss = criterion(outputs.logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # √âvaluation
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in val_loader:
            inputs, masks, labels = [x.to(device) for x in batch]
            outputs = model(inputs, attention_mask=masks)
            preds = torch.argmax(outputs.logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    print(f"\nEpoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}")
    print(classification_report(
        all_labels,
        all_preds,
        target_names=['Non Viral', 'Viral'],
        zero_division=0
    ))

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

!pip install transformers[torch] # Assurez-vous d'installer avec le support torch
# Importations
import pandas as pd
import re
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification, AutoConfig # Importez AutoConfig ici
from torch.optim import AdamW
import os # Import the os module


# 1. Fonction de nettoyage (doit √™tre d√©finie avant le chargement)
def clean_text(text):
    """Fonction de nettoyage des tweets"""
    text = re.sub(r'@\w+|#|http\S+|[^\w\s]', '', text.lower())
    return text.strip()

# 2. Localisation du fichier de poids
# Sp√©cifiez le chemin correct pour votre fichier .pth
WEIGHTS_PATH = "bert_balanced.pth"  # ou le nom de votre fichier de mod√®le sauvegard√©

# V√©rifiez si le fichier existe, d√©clenchez FileNotFoundError s'il n'est pas trouv√©
if not os.path.exists(WEIGHTS_PATH):
    raise FileNotFoundError(f"Aucun fichier .pth trouv√© √† l'emplacement : {WEIGHTS_PATH}. V√©rifiez votre sauvegarde.")

print(f"Chargement des poids depuis : {WEIGHTS_PATH}")

# 3. Configuration du mod√®le
MODEL_PATH = "bert_streamlit"
os.makedirs(MODEL_PATH, exist_ok=True)

# 4. Chargement s√©curis√© avec gestion des erreurs
try:
    # Autorisation explicite de la fonction clean_text
    with torch.serialization.safe_globals([clean_text]):
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Chargement du mod√®le de base
        model = BertForSequenceClassification.from_pretrained(
            'bert-base-uncased',
            num_labels=2
        ).to(device)

        # Chargement des poids avec v√©rification compl√®te
        state_dict = torch.load(
            WEIGHTS_PATH,
            map_location=device,
            # weights_only=False  # Modification ici : retirer weights_only
        )

        # V√©rification de la compatibilit√© des poids
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)

        if missing_keys:
            print(f"Attention : cl√©s manquantes - {missing_keys}")
        if unexpected_keys:
            print(f"Attention : cl√©s inattendues - {unexpected_keys}")

        model.eval()

except Exception as e:
    raise RuntimeError(f"Erreur de chargement : {str(e)}")

# 5. Sauvegarde compl√®te
model.save_pretrained(MODEL_PATH)
BertTokenizer.from_pretrained('bert-base-uncased').save_pretrained(MODEL_PATH)

# **Modification ici:** Sauvegarde explicite de l'√©tat du mod√®le
torch.save(model.state_dict(), os.path.join(MODEL_PATH, 'pytorch_model.bin'))

# Enregistrement de la configuration du mod√®le (pour compatibilit√©)
config = AutoConfig.from_pretrained(model.config._name_or_path) # Obtenir la configuration
config.save_pretrained(MODEL_PATH) # Sauvegarder la config dans le r√©pertoire

# Enregistrez le tokenizer dans le m√™me r√©pertoire.
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer.save_pretrained(MODEL_PATH)

# 6. M√©tadonn√©es avec s√©rialisation s√©curis√©e
metadata = {
    'class_names': ['Non Viral', 'Viral'],
    'max_length': 128,
    'clean_text_fn': clean_text  # R√©f√©rence √† la fonction d√©finie plus haut
}

torch.save(metadata, f"{MODEL_PATH}/metadata.pth", _use_new_zipfile_serialization=True)

# V√©rification finale
assert all(os.path.exists(f"{MODEL_PATH}/{f}") for f in [
    'pytorch_model.bin',
    'config.json',
    'metadata.pth'
]), "Erreur dans l'export des fichiers"

print("‚úÖ Export r√©ussi ! Structure :")
print("\n".join(sorted(os.listdir(MODEL_PATH))))

# EXPORT COMPLET POUR STREAMLIT
import torch
from transformers import BertForSequenceClassification, BertTokenizer
from safetensors.torch import save_file
import re
import os

# 1. CONFIGURATION
MODEL_DIR = "bert_streamlit_ready"  # Dossier de sortie
os.makedirs(MODEL_DIR, exist_ok=True)

# 2. CHARGEMENT DU MOD√àLE (remplacez par votre chemin)
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2
)
model.load_state_dict(torch.load('bert_balanced.pth'))  # Vos poids entra√Æn√©s
model.eval()

# 3. TOKENIZER
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# 4. FONCTION DE NETTOYAGE (identique √† l'entra√Ænement)
def clean_text(text):
    text = re.sub(r'@\w+|#|http\S+|[^\w\s]', '', text.lower())
    return text.strip()

# 5. SAUVEGARDE OPTIMIS√âE
# Mod√®le et tokenizer
model.save_pretrained(MODEL_DIR)
tokenizer.save_pretrained(MODEL_DIR)

# Conversion en safetensors (format recommand√©)
save_file(model.state_dict(), f"{MODEL_DIR}/model.safetensors")

# M√©tadonn√©es
torch.save({
    'class_names': ['Non Viral', 'Viral'],
    'max_length': 128,
    'clean_text_fn': clean_text
}, f"{MODEL_DIR}/metadata.pth")

# 6. NETTOYAGE (supprime les doublons)
if os.path.exists(f"{MODEL_DIR}/pytorch_model.bin"):
    os.remove(f"{MODEL_DIR}/pytorch_model.bin")

# 7. V√âRIFICATION FINALE
required_files = [
    'config.json',
    'model.safetensors',
    'tokenizer_config.json',
    'special_tokens_map.json',
    'vocab.txt',
    'metadata.pth'
]

missing = [f for f in required_files if not os.path.exists(f"{MODEL_DIR}/{f}")]
assert not missing, f"Fichiers manquants : {missing}"

print("‚úÖ Export r√©ussi ! Structure :")
print("\n".join(sorted(os.listdir(MODEL_DIR))))

# app.py
import streamlit as st
from transformers import BertForSequenceClassification, BertTokenizer
from safetensors.torch import load_file
import torch
import re
import os # Import the os module


# Configuration
MODEL_DIR = "bert_streamlit_ready"

# Define clean_text function outside of load_model for pickling
def clean_text(text):
    text = re.sub(r'@\w+|#|http\S+|[^\w\s]', '', text.lower())
    return text.strip()

# Fonction de chargement
@st.cache_resource
def load_model():
    # Tokenizer
    tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)

    # M√©tadonn√©es
    # Use safe_globals to allow clean_text
    with torch.serialization.safe_globals([clean_text]):
        metadata = torch.load(f"{MODEL_DIR}/metadata.pth", map_location='cpu')

    # Mod√®le
    model = BertForSequenceClassification.from_pretrained(MODEL_DIR, num_labels=2) # Remove state_dict
    model.load_state_dict(load_file(f"{MODEL_DIR}/model.safetensors")) # Load state dict separately
    model.eval()

    return model, tokenizer, metadata

# Interface
st.title("üîÆ Pr√©diction de Tweets Viraux")
model, tokenizer, metadata = load_model()

user_input = st.text_area("Entrez un tweet :")
if st.button("Pr√©dire") and user_input:
    # Nettoyage
    text = metadata['clean_text_fn'](user_input)

    # Tokenisation
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)

    # Pr√©diction
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)

    # R√©sultats
    viral_prob = probs[0][1].item()
    st.metric("Probabilit√© d'√™tre viral", f"{viral_prob:.1%}")

    # Jauge visuelle
    st.progress(viral_prob)

    # Interpr√©tation
    threshold = 0.7
    if viral_prob > threshold:
        st.success("üî• Tweet viral potentiel !")
    else:
        st.info("üí§ Peu susceptible de devenir viral")

import streamlit as st
from transformers import BertForSequenceClassification, BertTokenizer
from safetensors.torch import load_file
import torch
import re
import sys

def main():
    # Configuration
    MODEL_DIR = "bert_streamlit_ready"

    @st.cache_resource
    def load_model():
        model = BertForSequenceClassification.from_pretrained(
            MODEL_DIR,
            state_dict=load_file(f"{MODEL_DIR}/model.safetensors")
        )
        tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)
        metadata = torch.load(f"{MODEL_DIR}/metadata.pth", map_location='cpu')
        return model, tokenizer, metadata

    # Interface
    st.title("üîÆ Pr√©diction de Tweets Viraux")
    model, tokenizer, metadata = load_model()

    user_input = st.text_area("Entrez un tweet :")
    if st.button("Pr√©dire") and user_input:
        text = metadata['clean_text_fn'](user_input)
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)

        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)

        viral_prob = probs[0][1].item()
        st.metric("Probabilit√© d'√™tre viral", f"{viral_prob:.1%}")
        st.progress(viral_prob)

        if viral_prob > 0.7:
            st.success("üî• Tweet viral potentiel !")
        else:
            st.info("üí§ Peu susceptible de devenir viral")

if __name__ == "__main__":
    # Solution pour les avertissements
    if len(sys.argv) == 1 or sys.argv[1] != "run":
        print("Veuillez ex√©cuter avec : streamlit run app.py")
    else:
        main()

app_code = """
import streamlit as st
from transformers import BertForSequenceClassification, BertTokenizer
from safetensors.torch import load_file
import torch
import re
import sys

# Define clean_text function outside of load_model and main for pickling
def clean_text(text):
    text = re.sub(r'@\w+|#|http\S+|[^\w\s]', '', text.lower())
    return text.strip()

def main():
    # Configuration
    MODEL_DIR = "bert_streamlit_ready"

    # Chargement du mod√®le
    @st.cache_resource
    def load_model():
        try:
            # 1. Load the model structure from the directory
            model = BertForSequenceClassification.from_pretrained(MODEL_DIR, num_labels=2)

            # 2. Load the state dict
            model.load_state_dict(load_file(f"{MODEL_DIR}/model.safetensors"))

            tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)
            # Use safe_globals to allow clean_text when loading metadata
            with torch.serialization.safe_globals([clean_text]):
                metadata = torch.load(f"{MODEL_DIR}/metadata.pth", map_location='cpu')
            return model, tokenizer, metadata
        except Exception as e:
            st.error(f"Erreur de chargement du mod√®le : {str(e)}")
            return None, None, None  # Return None values on error

    # Interface
    st.set_page_config(page_title="Pr√©diction de Tweets Viraux", layout="wide")
    st.title("üîÆ Pr√©diction de Tweets Viraux")

    model, tokenizer, metadata = load_model()

    # Check if model loaded successfully before proceeding
    if model is not None and tokenizer is not None and metadata is not None:
        with st.form("prediction_form"):
            user_input = st.text_area("Entrez un tweet :")
            submitted = st.form_submit_button("Pr√©dire")

            if submitted and user_input:
                with st.spinner("Analyse en cours..."):
                    try:
                        # Nettoyage et tokenisation
                        text = metadata['clean_text_fn'](user_input)
                        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)

                        # Pr√©diction
                        with torch.no_grad():
                            outputs = model(**inputs)
                            probs = torch.softmax(outputs.logits, dim=1)

                        # Affichage
                        viral_prob = probs[0][1].item()
                        st.metric("Probabilit√© d'√™tre viral", f"{viral_prob:.1%}")
                        st.progress(viral_prob)

                        if viral_prob > 0.7:
                            st.success("üî• Tweet viral potentiel !")
                        else:
                            st.info("üí§ Peu susceptible de devenir viral")

                    except Exception as e:
                        st.error(f"Erreur lors de la pr√©diction : {str(e)}")
    else:
        st.error("Le mod√®le n'a pas pu √™tre charg√©. Veuillez v√©rifier les logs d'erreur.")



if __name__ == "__main__":
    if "streamlit" in sys.modules:
        main()
    else:
        print("‚ö†Ô∏è Veuillez ex√©cuter avec : streamlit run app.py")
        print("   Ou via Jupyter : !streamlit run app.py")

"""

with open("app.py", "w") as f:
    f.write(app_code)

print("‚úÖ Fichier app.py cr√©√© avec succ√®s !")

import streamlit as st
from transformers import BertForSequenceClassification, BertTokenizer
from safetensors.torch import load_file
import torch
import re
import sys
import warnings

# Suppression des avertissements Streamlit
warnings.filterwarnings("ignore", category=UserWarning, message="missing ScriptRunContext")

# Fonction de nettoyage globale (pour la s√©rialisation)
def clean_text(text):
    text = re.sub(r'@\w+|#|http\S+|[^\w\s]', '', text.lower())
    return text.strip()

def main():
    # Configuration
    MODEL_DIR = "bert_streamlit_ready"

    # Chargement du mod√®le avec cache
    @st.cache_resource
    def load_components():
        try:
            # Mod√®le
            model = BertForSequenceClassification.from_pretrained(MODEL_DIR)
            model.load_state_dict(load_file(f"{MODEL_DIR}/model.safetensors"))

            # Tokenizer
            tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)

            # M√©tadonn√©es avec contexte s√©curis√©
            with torch.serialization.safe_globals([clean_text]):
                metadata = torch.load(f"{MODEL_DIR}/metadata.pth", map_location='cpu')

            return model, tokenizer, metadata
        except Exception as e:
            st.error(f"Erreur technique : {str(e)}")
            sys.exit(1)

    # Interface
    st.set_page_config(
        page_title="Pr√©diction de Tweets Viraux",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("üîÆ Analyse de Viralit√© des Tweets")
    model, tokenizer, metadata = load_components()

    # Formulaire de pr√©diction
    with st.form(key="prediction_form"):
        tweet = st.text_area("Collez votre tweet ici :", height=150)
        submit = st.form_submit_button("Analyser")

        if submit and tweet:
            with st.spinner("Analyse en cours..."):
                try:
                    # Pr√©traitement
                    text = clean_text(tweet)
                    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)

                    # Pr√©diction
                    with torch.no_grad():
                        logits = model(**inputs).logits
                        proba = torch.softmax(logits, dim=1)[0][1].item()

                    # R√©sultats
                    st.metric("Score de Viralit√©", f"{proba:.1%}")
                    st.progress(proba)

                    # Interpr√©tation
                    if proba > 0.75:
                        st.balloons()
                        st.success("üî• Fort potentiel viral !")
                    elif proba > 0.5:
                        st.warning("üìà Potentiel viral moyen")
                    else:
                        st.info("üìâ Faible probabilit√© de viralit√©")

                except Exception as e:
                    st.error(f"Erreur d'analyse : {e}")

if __name__ == "__main__":
    if not st.runtime.exists():
        print("Usage : streamlit run app.py")
    else:
        main()

!streamlit run app.py

