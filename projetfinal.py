# -*- coding: utf-8 -*-
"""projetFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1657kUfkl0Nj62eaX4Slr_cVrGBN1c92-
"""

import pandas as pd
import random

import pandas as pd
df = pd.read_csv("tweets_viraux2.csv")
df.info()

import pandas as pd
import re



# Extraire les hashtags
df['hashtags'] = df['text'].apply(lambda x: re.findall(r'#\w+', x))

df_exploded = df.explode('hashtags')

stats_hashtags = df_exploded.groupby('hashtags')['likes'].agg(
    ['count', 'mean', 'median', 'max']
).reset_index().sort_values('count', ascending=False)

# Renommer les colonnes
stats_hashtags.columns = ['Hashtag', 'Nombre de tweets', 'Likes moyens', 'Likes médians', 'Likes max']

print(stats_hashtags)

!pip install streamlit
import streamlit as st

st.title("Statistiques des Hashtags")
st.dataframe(stats_hashtags)

pip install transformers torch pandas scikit-learn

from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Charger le tokenizer et le modèle pré-entraîné
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 classes : viral ou non

def tokenize_tweet(tweet, max_len=128):
    return tokenizer.encode_plus(
        tweet,
        add_special_tokens=True,
        max_length=max_len,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )

# Exemple
tweet = "This AI news is amazing! #ArtificialIntelligence"
inputs = tokenize_tweet(tweet)

import re
from sklearn.model_selection import train_test_split # Import train_test_split


def clean_text(text):
    """Supprime les hashtags, les mentions et les caractères spéciaux du texte."""
    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Supprime les mentions
    text = re.sub(r'#', '', text)  # Supprime le symbole hashtag
    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)  # Supprime les caractères spéciaux
    text = re.sub(r'\s+', ' ', text).strip()  # Supprime les espaces supplémentaires
    return text

df['text_clean'] = df['text'].apply(clean_text)  # Applique la fonction de nettoyage à la colonne 'text'

# Maintenant, vous pouvez procéder au train_test_split :
X_train, X_val, y_train, y_val = train_test_split(
    df['text_clean'], df['viral'], test_size=0.2, random_state=42
)

import torch # import the torch module

torch.cuda.empty_cache()

torch.cuda.empty_cache()

!nvidia-smi  # Vérifiez que le GPU est utilisé et la mémoire libre

import tensorflow as tf
print("GPU disponible:", tf.config.list_physical_devices('GPU'))

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Lance une opération lourde pour tester le GPU
x = torch.randn(10000, 10000).to(device)
y = torch.matmul(x, x)

print("Opération terminée sur :", device)

!pip install transformers[torch] # Make sure you install with torch support
# Importations
import pandas as pd
import re
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW



# 1. Chargement et prétraitement des données
df = pd.read_csv("tweets_viraux2.csv")

def clean_text(text):
    """Nettoie le texte des mentions, hashtags et caractères spéciaux"""
    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Supprime les mentions
    text = re.sub(r'#', '', text)  # Supprime les hashtags
    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)  # Supprime les caractères spéciaux
    return re.sub(r'\s+', ' ', text).strip().lower()  # Espaces et minuscules

df['text_clean'] = df['text'].apply(clean_text)

# 2. Split des données
X_train, X_val, y_train, y_val = train_test_split(
    df['text_clean'], df['viral'], test_size=0.2, random_state=42
)

# 3. Tokenisation globale (optimisée)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(
    X_train.tolist(),
    truncation=True,
    padding='max_length',
    max_length=128,
    return_tensors='pt'
)
val_encodings = tokenizer(
    X_val.tolist(),
    truncation=True,
    padding='max_length',
    max_length=128,
    return_tensors='pt'
)

# 4. Préparation des DataLoaders
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_dataset = TensorDataset(
    train_encodings['input_ids'],
    train_encodings['attention_mask'],
    torch.tensor(y_train.values)
)
val_dataset = TensorDataset(
    val_encodings['input_ids'],
    val_encodings['attention_mask'],
    torch.tensor(y_val.values)
)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

# 5. Initialisation du modèle
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2
).to(device)
optimizer = AdamW(model.parameters(), lr=5e-5) # Use AdamW from transformers.optimization

# ==== NOUVEAU CODE À COPIER ====
from torch.utils.data import WeightedRandomSampler
from torch.nn import CrossEntropyLoss

# A. Calcul des poids pour rééquilibrer les classes
class_counts = df['viral'].value_counts().to_list()
weights = torch.tensor([1.0/class_counts[0], 1.0/class_counts[1]], device=device)

# B. Sampler pondéré
sampler = WeightedRandomSampler(
    weights=[weights[lab] for lab in y_train],
    num_samples=len(y_train),
    replacement=True
)

# C. DataLoader avec sampler
train_loader = DataLoader(
    train_dataset,
    batch_size=16,
    sampler=sampler,  # Remplace shuffle=True
    num_workers=2
)

# D. Loss et optimizer
criterion = CrossEntropyLoss(weight=weights)
optimizer = AdamW(model.parameters(), lr=2e-5)

# E. Boucle d'entraînement améliorée
for epoch in range(5):  # 5 epochs maintenant
    model.train()
    total_loss = 0

    for batch in train_loader:
        inputs, masks, labels = [x.to(device) for x in batch]

        optimizer.zero_grad()
        outputs = model(inputs, attention_mask=masks, labels=labels)
        loss = criterion(outputs.logits, labels)  # Loss pondérée
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # Évaluation
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in val_loader:
            inputs, masks, labels = [x.to(device) for x in batch]
            outputs = model(inputs, attention_mask=masks)
            preds = torch.argmax(outputs.logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Affichage des métriques
    print(f"\nEpoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}")
    print(classification_report(
        all_labels,
        all_preds,
        target_names=['Non Viral', 'Viral'],
        zero_division=0
    ))

# Sauvegarde
torch.save(model.state_dict(), 'bert_balanced.pth')
# ==== FIN DU CODE À COPIER ====

# Importations supplémentaires nécessaires
import torch.nn as nn
from torch.nn import functional as F
from imblearn.over_sampling import RandomOverSampler

# 1. Rééquilibrage des classes
print("Distribution originale :")
print(df['viral'].value_counts())

ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(
    df[['text_clean']],
    df['viral']
)

# 2. Nouveau split
X_train, X_val, y_train, y_val = train_test_split(
    X_resampled['text_clean'],
    y_resampled,
    test_size=0.2,
    random_state=42
)

# 3. Réinitialisation du modèle
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2
).to(device)

# 4. Implémentation corrigée de FocalLoss
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return loss.mean()

# 5. Initialisation des composants d'entraînement
criterion = FocalLoss()
optimizer = AdamW(model.parameters(), lr=2e-5)

# 6. Tokenisation des données rééquilibrées
train_encodings = tokenizer(
    X_train.tolist(),
    truncation=True,
    padding='max_length',
    max_length=128,
    return_tensors='pt'
)
val_encodings = tokenizer(
    X_val.tolist(),
    truncation=True,
    padding='max_length',
    max_length=128,
    return_tensors='pt'
)

# 7. Préparation des DataLoaders
train_dataset = TensorDataset(
    train_encodings['input_ids'],
    train_encodings['attention_mask'],
    torch.tensor(y_train.values)
)
val_dataset = TensorDataset(
    val_encodings['input_ids'],
    val_encodings['attention_mask'],
    torch.tensor(y_val.values)
)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

# 8. Boucle d'entraînement
for epoch in range(5):
    model.train()
    total_loss = 0

    for batch in train_loader:
        inputs, masks, labels = [x.to(device) for x in batch]

        optimizer.zero_grad()
        outputs = model(inputs, attention_mask=masks)
        loss = criterion(outputs.logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # Évaluation
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in val_loader:
            inputs, masks, labels = [x.to(device) for x in batch]
            outputs = model(inputs, attention_mask=masks)
            preds = torch.argmax(outputs.logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    print(f"\nEpoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}")
    print(classification_report(
        all_labels,
        all_preds,
        target_names=['Non Viral', 'Viral'],
        zero_division=0
    ))

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

!pip install transformers[torch] # Assurez-vous d'installer avec le support torch
# Importations
import pandas as pd
import re
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification, AutoConfig # Importez AutoConfig ici
from torch.optim import AdamW
import os # Import the os module


# 1. Fonction de nettoyage (doit être définie avant le chargement)
def clean_text(text):
    """Fonction de nettoyage des tweets"""
    text = re.sub(r'@\w+|#|http\S+|[^\w\s]', '', text.lower())
    return text.strip()

# 2. Localisation du fichier de poids
# Spécifiez le chemin correct pour votre fichier .pth
WEIGHTS_PATH = "bert_balanced.pth"  # ou le nom de votre fichier de modèle sauvegardé

# Vérifiez si le fichier existe, déclenchez FileNotFoundError s'il n'est pas trouvé
if not os.path.exists(WEIGHTS_PATH):
    raise FileNotFoundError(f"Aucun fichier .pth trouvé à l'emplacement : {WEIGHTS_PATH}. Vérifiez votre sauvegarde.")

print(f"Chargement des poids depuis : {WEIGHTS_PATH}")

# 3. Configuration du modèle
MODEL_PATH = "bert_streamlit"
os.makedirs(MODEL_PATH, exist_ok=True)

# 4. Chargement sécurisé avec gestion des erreurs
try:
    # Autorisation explicite de la fonction clean_text
    with torch.serialization.safe_globals([clean_text]):
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Chargement du modèle de base
        model = BertForSequenceClassification.from_pretrained(
            'bert-base-uncased',
            num_labels=2
        ).to(device)

        # Chargement des poids avec vérification complète
        state_dict = torch.load(
            WEIGHTS_PATH,
            map_location=device,
            # weights_only=False  # Modification ici : retirer weights_only
        )

        # Vérification de la compatibilité des poids
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)

        if missing_keys:
            print(f"Attention : clés manquantes - {missing_keys}")
        if unexpected_keys:
            print(f"Attention : clés inattendues - {unexpected_keys}")

        model.eval()

except Exception as e:
    raise RuntimeError(f"Erreur de chargement : {str(e)}")

# 5. Sauvegarde complète
model.save_pretrained(MODEL_PATH)
BertTokenizer.from_pretrained('bert-base-uncased').save_pretrained(MODEL_PATH)

# **Modification ici:** Sauvegarde explicite de l'état du modèle
torch.save(model.state_dict(), os.path.join(MODEL_PATH, 'pytorch_model.bin'))

# Enregistrement de la configuration du modèle (pour compatibilité)
config = AutoConfig.from_pretrained(model.config._name_or_path) # Obtenir la configuration
config.save_pretrained(MODEL_PATH) # Sauvegarder la config dans le répertoire

# Enregistrez le tokenizer dans le même répertoire.
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer.save_pretrained(MODEL_PATH)

# 6. Métadonnées avec sérialisation sécurisée
metadata = {
    'class_names': ['Non Viral', 'Viral'],
    'max_length': 128,
    'clean_text_fn': clean_text  # Référence à la fonction définie plus haut
}

torch.save(metadata, f"{MODEL_PATH}/metadata.pth", _use_new_zipfile_serialization=True)

# Vérification finale
assert all(os.path.exists(f"{MODEL_PATH}/{f}") for f in [
    'pytorch_model.bin',
    'config.json',
    'metadata.pth'
]), "Erreur dans l'export des fichiers"

print("✅ Export réussi ! Structure :")
print("\n".join(sorted(os.listdir(MODEL_PATH))))

# EXPORT COMPLET POUR STREAMLIT
import torch
from transformers import BertForSequenceClassification, BertTokenizer
from safetensors.torch import save_file
import re
import os

# 1. CONFIGURATION
MODEL_DIR = "bert_streamlit_ready"  # Dossier de sortie
os.makedirs(MODEL_DIR, exist_ok=True)

# 2. CHARGEMENT DU MODÈLE (remplacez par votre chemin)
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2
)
model.load_state_dict(torch.load('bert_balanced.pth'))  # Vos poids entraînés
model.eval()

# 3. TOKENIZER
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# 4. FONCTION DE NETTOYAGE (identique à l'entraînement)
def clean_text(text):
    text = re.sub(r'@\w+|#|http\S+|[^\w\s]', '', text.lower())
    return text.strip()

# 5. SAUVEGARDE OPTIMISÉE
# Modèle et tokenizer
model.save_pretrained(MODEL_DIR)
tokenizer.save_pretrained(MODEL_DIR)

# Conversion en safetensors (format recommandé)
save_file(model.state_dict(), f"{MODEL_DIR}/model.safetensors")

# Métadonnées
torch.save({
    'class_names': ['Non Viral', 'Viral'],
    'max_length': 128,
    'clean_text_fn': clean_text
}, f"{MODEL_DIR}/metadata.pth")

# 6. NETTOYAGE (supprime les doublons)
if os.path.exists(f"{MODEL_DIR}/pytorch_model.bin"):
    os.remove(f"{MODEL_DIR}/pytorch_model.bin")

# 7. VÉRIFICATION FINALE
required_files = [
    'config.json',
    'model.safetensors',
    'tokenizer_config.json',
    'special_tokens_map.json',
    'vocab.txt',
    'metadata.pth'
]

missing = [f for f in required_files if not os.path.exists(f"{MODEL_DIR}/{f}")]
assert not missing, f"Fichiers manquants : {missing}"

print("✅ Export réussi ! Structure :")
print("\n".join(sorted(os.listdir(MODEL_DIR))))

# app.py
import streamlit as st
from transformers import BertForSequenceClassification, BertTokenizer
from safetensors.torch import load_file
import torch
import re
import os # Import the os module


# Configuration
MODEL_DIR = "bert_streamlit_ready"

# Define clean_text function outside of load_model for pickling
def clean_text(text):
    text = re.sub(r'@\w+|#|http\S+|[^\w\s]', '', text.lower())
    return text.strip()

# Fonction de chargement
@st.cache_resource
def load_model():
    # Tokenizer
    tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)

    # Métadonnées
    # Use safe_globals to allow clean_text
    with torch.serialization.safe_globals([clean_text]):
        metadata = torch.load(f"{MODEL_DIR}/metadata.pth", map_location='cpu')

    # Modèle
    model = BertForSequenceClassification.from_pretrained(MODEL_DIR, num_labels=2) # Remove state_dict
    model.load_state_dict(load_file(f"{MODEL_DIR}/model.safetensors")) # Load state dict separately
    model.eval()

    return model, tokenizer, metadata

# Interface
st.title("🔮 Prédiction de Tweets Viraux")
model, tokenizer, metadata = load_model()

user_input = st.text_area("Entrez un tweet :")
if st.button("Prédire") and user_input:
    # Nettoyage
    text = metadata['clean_text_fn'](user_input)

    # Tokenisation
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)

    # Prédiction
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)

    # Résultats
    viral_prob = probs[0][1].item()
    st.metric("Probabilité d'être viral", f"{viral_prob:.1%}")

    # Jauge visuelle
    st.progress(viral_prob)

    # Interprétation
    threshold = 0.7
    if viral_prob > threshold:
        st.success("🔥 Tweet viral potentiel !")
    else:
        st.info("💤 Peu susceptible de devenir viral")

import streamlit as st
from transformers import BertForSequenceClassification, BertTokenizer
from safetensors.torch import load_file
import torch
import re
import sys

def main():
    # Configuration
    MODEL_DIR = "bert_streamlit_ready"

    @st.cache_resource
    def load_model():
        model = BertForSequenceClassification.from_pretrained(
            MODEL_DIR,
            state_dict=load_file(f"{MODEL_DIR}/model.safetensors")
        )
        tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)
        metadata = torch.load(f"{MODEL_DIR}/metadata.pth", map_location='cpu')
        return model, tokenizer, metadata

    # Interface
    st.title("🔮 Prédiction de Tweets Viraux")
    model, tokenizer, metadata = load_model()

    user_input = st.text_area("Entrez un tweet :")
    if st.button("Prédire") and user_input:
        text = metadata['clean_text_fn'](user_input)
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)

        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)

        viral_prob = probs[0][1].item()
        st.metric("Probabilité d'être viral", f"{viral_prob:.1%}")
        st.progress(viral_prob)

        if viral_prob > 0.7:
            st.success("🔥 Tweet viral potentiel !")
        else:
            st.info("💤 Peu susceptible de devenir viral")

if __name__ == "__main__":
    # Solution pour les avertissements
    if len(sys.argv) == 1 or sys.argv[1] != "run":
        print("Veuillez exécuter avec : streamlit run app.py")
    else:
        main()

app_code = """
import streamlit as st
from transformers import BertForSequenceClassification, BertTokenizer
from safetensors.torch import load_file
import torch
import re
import sys

# Define clean_text function outside of load_model and main for pickling
def clean_text(text):
    text = re.sub(r'@\w+|#|http\S+|[^\w\s]', '', text.lower())
    return text.strip()

def main():
    # Configuration
    MODEL_DIR = "bert_streamlit_ready"

    # Chargement du modèle
    @st.cache_resource
    def load_model():
        try:
            # 1. Load the model structure from the directory
            model = BertForSequenceClassification.from_pretrained(MODEL_DIR, num_labels=2)

            # 2. Load the state dict
            model.load_state_dict(load_file(f"{MODEL_DIR}/model.safetensors"))

            tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)
            # Use safe_globals to allow clean_text when loading metadata
            with torch.serialization.safe_globals([clean_text]):
                metadata = torch.load(f"{MODEL_DIR}/metadata.pth", map_location='cpu')
            return model, tokenizer, metadata
        except Exception as e:
            st.error(f"Erreur de chargement du modèle : {str(e)}")
            return None, None, None  # Return None values on error

    # Interface
    st.set_page_config(page_title="Prédiction de Tweets Viraux", layout="wide")
    st.title("🔮 Prédiction de Tweets Viraux")

    model, tokenizer, metadata = load_model()

    # Check if model loaded successfully before proceeding
    if model is not None and tokenizer is not None and metadata is not None:
        with st.form("prediction_form"):
            user_input = st.text_area("Entrez un tweet :")
            submitted = st.form_submit_button("Prédire")

            if submitted and user_input:
                with st.spinner("Analyse en cours..."):
                    try:
                        # Nettoyage et tokenisation
                        text = metadata['clean_text_fn'](user_input)
                        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)

                        # Prédiction
                        with torch.no_grad():
                            outputs = model(**inputs)
                            probs = torch.softmax(outputs.logits, dim=1)

                        # Affichage
                        viral_prob = probs[0][1].item()
                        st.metric("Probabilité d'être viral", f"{viral_prob:.1%}")
                        st.progress(viral_prob)

                        if viral_prob > 0.7:
                            st.success("🔥 Tweet viral potentiel !")
                        else:
                            st.info("💤 Peu susceptible de devenir viral")

                    except Exception as e:
                        st.error(f"Erreur lors de la prédiction : {str(e)}")
    else:
        st.error("Le modèle n'a pas pu être chargé. Veuillez vérifier les logs d'erreur.")



if __name__ == "__main__":
    if "streamlit" in sys.modules:
        main()
    else:
        print("⚠️ Veuillez exécuter avec : streamlit run app.py")
        print("   Ou via Jupyter : !streamlit run app.py")

"""

with open("app.py", "w") as f:
    f.write(app_code)

print("✅ Fichier app.py créé avec succès !")

import streamlit as st
from transformers import BertForSequenceClassification, BertTokenizer
from safetensors.torch import load_file
import torch
import re
import sys
import warnings

# Suppression des avertissements Streamlit
warnings.filterwarnings("ignore", category=UserWarning, message="missing ScriptRunContext")

# Fonction de nettoyage globale (pour la sérialisation)
def clean_text(text):
    text = re.sub(r'@\w+|#|http\S+|[^\w\s]', '', text.lower())
    return text.strip()

def main():
    # Configuration
    MODEL_DIR = "bert_streamlit_ready"

    # Chargement du modèle avec cache
    @st.cache_resource
    def load_components():
        try:
            # Modèle
            model = BertForSequenceClassification.from_pretrained(MODEL_DIR)
            model.load_state_dict(load_file(f"{MODEL_DIR}/model.safetensors"))

            # Tokenizer
            tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)

            # Métadonnées avec contexte sécurisé
            with torch.serialization.safe_globals([clean_text]):
                metadata = torch.load(f"{MODEL_DIR}/metadata.pth", map_location='cpu')

            return model, tokenizer, metadata
        except Exception as e:
            st.error(f"Erreur technique : {str(e)}")
            sys.exit(1)

    # Interface
    st.set_page_config(
        page_title="Prédiction de Tweets Viraux",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("🔮 Analyse de Viralité des Tweets")
    model, tokenizer, metadata = load_components()

    # Formulaire de prédiction
    with st.form(key="prediction_form"):
        tweet = st.text_area("Collez votre tweet ici :", height=150)
        submit = st.form_submit_button("Analyser")

        if submit and tweet:
            with st.spinner("Analyse en cours..."):
                try:
                    # Prétraitement
                    text = clean_text(tweet)
                    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)

                    # Prédiction
                    with torch.no_grad():
                        logits = model(**inputs).logits
                        proba = torch.softmax(logits, dim=1)[0][1].item()

                    # Résultats
                    st.metric("Score de Viralité", f"{proba:.1%}")
                    st.progress(proba)

                    # Interprétation
                    if proba > 0.75:
                        st.balloons()
                        st.success("🔥 Fort potentiel viral !")
                    elif proba > 0.5:
                        st.warning("📈 Potentiel viral moyen")
                    else:
                        st.info("📉 Faible probabilité de viralité")

                except Exception as e:
                    st.error(f"Erreur d'analyse : {e}")

if __name__ == "__main__":
    if not st.runtime.exists():
        print("Usage : streamlit run app.py")
    else:
        main()

!streamlit run app.py

