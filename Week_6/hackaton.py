# -*- coding: utf-8 -*-
"""Hackatonfinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kw-G8TWQKzPRkXTeXmOqWFSHYnBl6aeZ
"""



"""‚úÖ √âtape 1 ‚Äì R√©cup√©ration des articles depuis Google News
On va utiliser la biblioth√®que GoogleNews ou bien gnews (plus stable et simple). Elle permet de rechercher des articles par mot-cl√©, langue, pays, et date.

1. Installation des biblioth√®ques (dans Google Colab) :
"""

!pip install gnews

from gnews import GNews

# Configurer la recherche
google_news = GNews(language='fr', country='FR', period='7d', max_results=10)

# Rechercher des articles sur l'intelligence artificielle
articles = google_news.get_news('intelligence artificielle')

# Afficher les titres et liens
for article in articles:
    print(f"Titre : {article['title']}")
    print(f"Date : {article['published date']}")
    print(f"Lien : {article['url']}")
    print("-----")

"""## üîç Pourquoi ce filtrage d'articles ?

Lors d‚Äôun hackathon, le temps est limit√© et on cherche √† √™tre le plus efficace possible.  
Or, de nombreux sites d‚Äôactualit√©s ne peuvent pas √™tre facilement scrapp√©s car ils sont prot√©g√©s par :

- des **paywalls** (acc√®s r√©serv√© aux abonn√©s),
- des **captcha ou protections anti-bot** (Cloudflare, etc.),
- des **codes d‚Äôerreur 403 ou 404** lors de requ√™tes simples.

‚û°Ô∏è Pour gagner du temps et √©viter de coder des contournements complexes, nous avons fait le choix de **filtrer uniquement les articles accessibles directement via une requ√™te HTTP simple (code 200)**.

Cela permet de :

- Prototyper rapidement un pipeline fonctionnel,
- Travailler avec des articles pertinents,
- Se concentrer sur les √©tapes d‚Äô**analyse, de r√©sum√© et de g√©n√©ration**.

Ce compromis est adapt√© √† un contexte de hackathon rapide, tout en gardant une base extensible pour le futur.

"""

import requests

# Fonction pour tester si un article est accessible
def is_accessible(url):
    try:
        response = requests.get(url, timeout=5, headers={"User-Agent": "Mozilla/5.0"})
        return response.status_code == 200
    except:
        return False

# Filtrer les articles accessibles uniquement
accessible_articles = [a for a in articles if is_accessible(a['url'])]

# Afficher les articles valides
print(f"‚úÖ {len(accessible_articles)} articles accessibles :\n")
for a in accessible_articles:
    print(f"Titre : {a['title']}")
    print(f"Date  : {a['published date']}")
    print(f"Lien  : {a['url']}")
    print("------")

"""‚úÖ √âtape 2 : R√©cup√©rer le contenu des articles
Maintenant qu'on a les liens, on va essayer de r√©cup√©rer automatiquement le texte complet des articles (ce qui va nous servir √† r√©sumer et reformuler ensuite).

On va utiliser newspaper3k, une librairie super pratique pour extraire le texte propre d‚Äôun article √† partir de son URL.
"""

# Installer la librairie (si ce n'est pas d√©j√† fait)
!pip install newspaper3k

from newspaper import Article

# Exemple avec le premier article
url = articles[0]['url']
article = Article(url)

# T√©l√©charger et parser le contenu
article.download()
article.parse()

# Affichage du texte brut
print("üì∞ Contenu brut de l‚Äôarticle :")
print(article.text[:1000])  # On affiche les 1000 premiers caract√®res

"""il bnettoie pour reprendre des balises en html"""

!pip install lxml_html_clean

# Installer la librairie (si ce n'est pas d√©j√† fait)
!pip install newspaper3k

from newspaper import Article

# Exemple avec le premier article
url = articles[0]['url']
article = Article(url)

# T√©l√©charger et parser le contenu
article.download()
article.parse()

# Affichage du texte brut
print("üì∞ Contenu brut de l‚Äôarticle :")
print(article.text[:1000])  # On affiche les 1000 premiers caract√®res

"""from transformers import BartForConditionalGeneration, BartTokenizer
import torch

# Charger le mod√®le et le tokenizer BART pr√©-entra√Æn√© pour le r√©sum√©
model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# Limiter la taille du texte pour rester dans les limites du mod√®le
text = article.text[:1024]

# Tokenization du texte
inputs = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)

# G√©n√©ration du r√©sum√©
summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)

# D√©codage du r√©sum√©
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Affichage
print("üìù R√©sum√© de l'article :\n")
print(summary)

"""

from transformers import BartForConditionalGeneration, BartTokenizer
import torch

# Charger le mod√®le et le tokenizer BART pr√©-entra√Æn√© pour le r√©sum√©
model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# Limiter la taille du texte pour rester dans les limites du mod√®le
text = article.text[:1024]

# Tokenization du texte
inputs = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)

# G√©n√©ration du r√©sum√©
summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)

# D√©codage du r√©sum√©
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Affichage
print("üìù R√©sum√© de l'article :\n")
print(summary)

print(len(article.text))
print(article.text[:500])  # Pour voir les 500 premiers caract√®res

article = Article(articles[1]['url'])
article.download()
article.parse()

from newspaper import Article

# Trouver le 1er article avec du texte pertinent
def trouver_article_valide(articles, min_length=500):
    for i, article_info in enumerate(articles):
        print(f"üîé Test de l'article {i+1} : {article_info['title']}")
        url = article_info['url']
        try:
            article = Article(url)
            article.download()
            article.parse()

            if len(article.text) >= min_length:
                print(f"‚úÖ Article {i+1} OK : {len(article.text)} caract√®res trouv√©s.\n")
                return article
            else:
                print(f"‚ö†Ô∏è Trop court ({len(article.text)} caract√®res)\n")
        except Exception as e:
            print(f"‚ùå Erreur sur l'article {i+1} : {e}\n")

    print("‚ùå Aucun article avec assez de contenu trouv√©.")
    return None

article = trouver_article_valide(articles)

if article:
    print("üì∞ Aper√ßu du texte :")
    print(article.text[:500])
else:
    print("Aucun article n'a pu √™tre utilis√©.")

# Installation des librairies n√©cessaires (√† ex√©cuter une seule fois)
!pip install gnews newspaper3k

# Imports
from gnews import GNews
from newspaper import Article
from urllib.parse import urlparse, parse_qs

# üîß Fonction pour extraire le vrai lien depuis un lien Google News
def extraire_vrai_lien(url_google_news):
    parsed_url = urlparse(url_google_news)
    params = parse_qs(parsed_url.query)
    return params.get("url", [url_google_news])[0]

# üîé Configuration de la recherche
google_news = GNews(language='fr', country='FR', period='7d', max_results=10)
articles_info = google_news.get_news('intelligence artificielle')

# üîÑ Boucle sur les articles pour trouver le premier exploitable
article_utilisable = None

for idx, article_info in enumerate(articles_info):
    print(f"üîé Test de l'article {idx+1} : {article_info['title']}")

    try:
        url = extraire_vrai_lien(article_info['url'])
        article = Article(url)
        article.download()
        article.parse()

        if len(article.text.strip()) > 500:
            print("‚úÖ Article avec assez de contenu trouv√© !\n")
            article_utilisable = article
            break
        else:
            print("‚ö†Ô∏è Trop court (0 caract√®res)\n")

    except Exception as e:
        print(f"‚ùå Erreur pendant l'extraction : {e}\n")

# üñ®Ô∏è Affichage du r√©sultat
if article_utilisable:
    print("üì∞ Aper√ßu du texte extrait :\n")
    print(article_utilisable.text[:1000])
else:
    print("‚ùå Aucun article avec assez de contenu trouv√©.")

"""üß† Objectif
Cr√©er un scraper intelligent qui :

contourne les protections classiques (ex : JavaScript, anti-bot)
extrait le texte complet m√™me quand newspaper3k √©choue
s‚Äôint√®gre dans ton pipeline actuel (GNews + r√©sum√© + Notion)
üõ†Ô∏è Solution propos√©e : Utiliser Playwright
Playwright permet de contr√¥ler un vrai navigateur (comme Chrome ou Firefox), ce qui contourne les blocages JavaScript ou Cloudflare.
Avantages :

Rendu complet de la page (y compris JavaScript)
Moins de blocages anti-scraping
Peut fonctionner dans une boucle pour tester tous les liens
√âtapes :

On teste chaque lien avec newspaper3k.
Si le texte est vide ou trop court, on utilise Playwright pour "r√©cup√©rer le vrai texte".
On continue le pipeline (r√©sum√© ‚Üí Notion).
"""

!pip install playwright
!playwright install

"""üß™ On installe Selenium avec un Chrome "headless" sur Colab car la solution d'au dessus ne marche pas sur collab"""

!apt-get update
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
!pip install selenium
import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)
    driver.get(url)

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    text = soup.get_text()

    driver.quit()
    return text

url = "https://www.vie-publique.fr/actualite/294705-lintelligence-artificielle-un-outil-de-conduite-des-politiques-publiques"

texte = get_text_with_selenium(url)
print(texte[:1500])  # Affiche les 1500 premiers caract√®res du contenu

"""‚úÖ √âtape suivante : cliquer automatiquement sur "Accepter les cookies"
Pour r√©cup√©rer le vrai contenu, on doit simuler un clic sur le bouton "Accepter" avant d‚Äôextraire le texte. car la il s'arrete a la page de rpesentation.

Voici un exemple de code √† tester dans ta cellule actuelle :
"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)
    driver.get(url)

    try:
        # Accepter les cookies si le bouton est pr√©sent
        accept_button = WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Accepter')]"))
        )
        accept_button.click()
    except:
        print("Pas de bouton cookies d√©tect√©.")

    # Attendre le chargement du contenu
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.TAG_NAME, "article"))  # ou remplacer par une div sp√©cifique si besoin
    )

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    text = soup.get_text()

    driver.quit()
    return text

text = get_text_with_selenium("https://www.vie-publique.fr/en-bref/293466-intelligence-artificielle-un-outil-de-conduite-des-politiques-publiques")
print(text[:1500])  # Pour afficher un extrait

"""Tu es tomb√© sur une TimeoutException. Cela signifie que la commande Selenium a attendu trop longtemps sans trouver l‚Äô√©l√©ment demand√© (ici probablement le bouton "Accepter" ou l‚Äô√©l√©ment de contenu principal).

‚úÖ On corrige √ßa √©tape par √©tape :
1. Enlever temporairement l‚Äôattente d‚Äôun √©l√©ment pr√©cis

On va d‚Äôabord tester que le chargement de la page se fait bien, sans chercher d‚Äô√©l√©ment sp√©cifique :
"""

def get_text_with_selenium_simple(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)
    driver.get(url)

    # Attente simple de 5 secondes pour charger la page
    import time
    time.sleep(5)

    # Extraction du texte complet
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    text = soup.get_text()

    driver.quit()
    return text

text = get_text_with_selenium_simple("https://www.vie-publique.fr/en-bref/293466-intelligence-artificielle-un-outil-de-conduite-des-politiques-publiques")
print(text[:1500])

"""au final on se rend compte que le site choisi n'etait pas exploitable je decide donc de lui donner moi meme l'url d'un article interessant que l'on va utiliser"""

import requests
from bs4 import BeautifulSoup

# Lien de l'article
url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"

# En-t√™tes pour simuler un vrai navigateur (important !)
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# Requ√™te HTTP
response = requests.get(url, headers=headers)

# Parsing HTML
soup = BeautifulSoup(response.text, "html.parser")

# Extraire les paragraphes de l'article
paragraphs = soup.find_all("p")
content = "\n\n".join([p.get_text() for p in paragraphs])

# Aper√ßu
print("üì∞ Extrait du contenu :\n")
print(content[:1500])  # Affiche les 1500 premiers caract√®res

"""Parfait si requests a fonctionn√© avec cet article‚ÄØ! üéØ
√áa veut dire que le site Si√®cle Digital ne bloque pas les scrapers classiques, donc on peut bosser plus rapidement et efficacement.

üé¨ √âtape suivante : r√©sumer le contenu avec BART
Voici le code complet pour faire :

üì• T√©l√©charger l‚Äôarticle avec requests
üß† G√©n√©rer un r√©sum√© automatique avec le mod√®le facebook/bart-large-cnn
"""

import requests
from bs4 import BeautifulSoup
from transformers import BartTokenizer, BartForConditionalGeneration

# URL de l'article
url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"

# √âtape 1 : T√©l√©charger et parser l'article
headers = {'User-Agent': 'Mozilla/5.0'}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.text, 'html.parser')

# Extraire le texte principal (on cible les balises <p>)
paragraphs = soup.find_all('p')
full_text = ' '.join([p.get_text() for p in paragraphs])
text = full_text[:1024]  # BART accepte jusqu'√† 1024 tokens

# √âtape 2 : G√©n√©rer un r√©sum√© avec BART
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

inputs = tokenizer.encode(text, return_tensors='pt', max_length=1024, truncation=True)
summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print("üß† R√©sum√© g√©n√©r√© :")
print(summary)

"""üß† BART = Bidirectional and Auto-Regressive Transformers
C‚Äôest un mod√®le de langage d√©velopp√© par Facebook AI (Meta), sp√©cialis√© dans la g√©n√©ration de texte (r√©sum√©, correction, paraphrase‚Ä¶).

‚úÖ √Ä quoi √ßa sert ?
R√©sumer un texte üìÑ
Traduire üåç
Corriger du texte ‚úçÔ∏è
G√©n√©rer une suite coh√©rente üîÅ
R√©√©crire/vulgariser un contenu üßë‚Äçüè´
‚öôÔ∏è Comment √ßa fonctionne ?
BART combine :

Encodeur bidirectionnel (comme BERT) ‚Üí il lit tout le texte √† la fois pour comprendre le contexte.
D√©codeur auto-r√©gressif (comme GPT) ‚Üí il g√©n√®re mot par mot, un peu comme un √©crivain.
‚û°Ô∏è Donc, il comprend bien le texte (comme BERT)
‚û°Ô∏è Et il √©crit bien des r√©sum√©s ou du nouveau contenu (comme GPT)

üìö En r√©sum√© :
BART prend un texte long et en fait une version plus courte, claire et coh√©rente, tout en gardant le sens essentiel.
C‚Äôest parfait pour ton pipeline d'articles IA !

"""



"""Parfait ! üéØ On va cr√©er une fonction automatis√©e qui transforme un r√©sum√© brut (comme celui g√©n√©r√© par BART) en post LinkedIn structur√©, clair et accrocheur, avec un ton p√©dagogique et humain.

‚úÖ √âtape 1 : Fonction de transformation du r√©sum√© en post LinkedIn
"""



def generer_post_linkedin(resume, sujet="intelligence artificielle"):
    prompt = f"""
Tu es un expert en vulgarisation tech & IA sur LinkedIn. √Ä partir du r√©sum√© suivant, g√©n√®re un post structur√©, p√©dagogique et engageant :

R√©sum√© : "{resume}"

Structure souhait√©e :
1. Accroche forte pour captiver le lecteur
2. Explication claire du probl√®me ou sujet
3. Points cl√©s √† retenir (bullet points si possible)
4. Une phrase de conclusion ou une question ouverte pour susciter l‚Äôengagement

Le style doit rester simple, professionnel, accessible √† un public non expert. Ajoute des emojis si pertinents, sans en abuser. N‚Äôinvente pas de faits.

Sujet : {sujet}
"""

    return prompt

"""me connecter et creer une clef api open AI"""

import openai

openai.api_key = "sk-..."  # ta cl√© OpenAI

prompt = generer_post_linkedin(summary)

response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}],
    temperature=0.7,
    max_tokens=500
)

linkedin_post = response['choices'][0]['message']['content']
print(linkedin_post)

"""je dois me creer une clef api open ai"""

import openai

# Ta cl√© API OpenAI ici (remplace entre les guillemets)
openai.api_key = "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"  # üëà Mets ta cl√© ici

# R√©sum√© g√©n√©r√© avec BART
resume = """
Une √©tude r√©cente du Tow Center for Digital Journalism de l‚Äôuniversit√© Columbia met en lumi√®re un probl√®me majeur des moteurs de recherche dop√©s. Plus de 60% des requ√™tes li√©es aux actualit√©s recevraient des r√©ponses erron√©es.
"""

# Prompt pour transformer en post LinkedIn vulgaris√©
prompt = f"""
Tu es un expert en vulgarisation d‚Äôactualit√©s IA pour LinkedIn. Reformule ce r√©sum√© pour en faire un post LinkedIn engageant, structur√© ainsi :
- Intro courte avec une accroche claire
- Contexte de l‚Äô√©tude
- Ce qu‚Äôil faut retenir
- Question ou appel √† r√©action pour conclure

R√©sum√© :
{resume}
"""

# Appel √† l‚ÄôAPI OpenAI
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",  # ou "gpt-4" si tu y as acc√®s
    messages=[
        {"role": "user", "content": prompt}
    ],
    temperature=0.7,
    max_tokens=400
)

# Affichage du post LinkedIn
linkedin_post = response["choices"][0]["message"]["content"]
print("üí¨ Post LinkedIn g√©n√©r√© :\n")
print(linkedin_post)

"""api trop ancienne donc on prend une plus recente"""

from openai import OpenAI

client = OpenAI(api_key="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")  # Mets ta cl√© ici

# R√©sum√© √† transformer
resume = """
Une √©tude r√©cente du Tow Center for Digital Journalism de l‚Äôuniversit√© Columbia met en lumi√®re un probl√®me majeur des moteurs de recherche dop√©s. Plus de 60% des requ√™tes li√©es aux actualit√©s recevraient des r√©ponses erron√©es.
"""

prompt = f"""
Tu es un expert en vulgarisation d‚Äôactualit√©s IA pour LinkedIn. Reformule ce r√©sum√© pour en faire un post LinkedIn engageant, structur√© ainsi :
- Intro courte avec une accroche claire
- Contexte de l‚Äô√©tude
- Ce qu‚Äôil faut retenir
- Question ou appel √† r√©action pour conclure

R√©sum√© :
{resume}
"""

# Appel √† l‚ÄôAPI avec la nouvelle m√©thode
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": prompt}
    ],
    temperature=0.7,
    max_tokens=400
)

# Affichage du contenu LinkedIn
linkedin_post = response.choices[0].message.content
print("üí¨ Post LinkedIn g√©n√©r√© :\n")
print(linkedin_post)

!pip install notion-client

from notion_client import Client
from datetime import datetime

# ‚úÖ Cl√© API Notion
notion = Client(auth="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")  # remplace ici ta cl√©

# ‚úÖ ID de ta base
database_id = "1bf902baf626806ea938db3de3c3be4d"

# ‚úÖ Donn√©es √† envoyer (exemple)
titre_article = "üìå √âtude : plus de 60% d'erreurs dans les r√©ponses IA sur l'actualit√©"
linkedin_post = """Voici un article qui nous alerte : plus de 60% des r√©ponses g√©n√©r√©es par des IA aux questions d‚Äôactualit√© seraient erron√©es. ‚ùå

üëâ Cette √©tude du Tow Center de l‚ÄôUniversit√© Columbia r√©v√®le les limites actuelles des IA dans le domaine de l'information en temps r√©el.

‚úÖ Ce qu‚Äôil faut retenir :
- Les mod√®les peinent √† restituer des faits √† jour
- Le public peut √™tre induit en erreur
- Il est crucial de valider les sources et d'int√©grer un filtre de v√©rification

üß† En r√©sum√© : l‚ÄôIA, oui ‚Äî mais pas sans garde-fous, surtout sur l‚Äôinfo sensible et mouvante !"""

# ‚úÖ Envoi √† Notion
notion.pages.create(
    parent={"database_id": database_id},
    properties={
        "Titre": {
            "title": [{"text": {"content": titre_article}}]
        },
        "Contenu": {
            "rich_text": [{"text": {"content": linkedin_post}}]
        },
        "Date": {
            "date": {"start": str(datetime.today())}
        }
    }
)

print("‚úÖ Article envoy√© dans Notion avec succ√®s !")

from notion_client import Client
from datetime import datetime

# üîê Cl√© API Notion (ne pas partager)
notion = Client(auth="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")

# üÜî Remplace par l‚ÄôID de ta base de donn√©es Notion
database_id = "1bf902baf626806ea938db3de3c3be4d"

# üìÑ Donn√©es de test √† envoyer
mon_titre = "Article g√©n√©r√© automatiquement"
mon_texte = "Voici le texte g√©n√©r√© automatiquement pour LinkedIn"
date_article = datetime.today().strftime('%Y-%m-%d')

# üì§ Requ√™te vers l'API Notion
notion.pages.create(
    parent={"database_id": database_id},
    properties={
        "Name": {
            "title": [
                {
                    "text": {
                        "content": mon_titre
                    }
                }
            ]
        },
        "Contenu": {
            "rich_text": [
                {
                    "text": {
                        "content": mon_texte
                    }
                }
            ]
        },
        "Date": {
            "date": {
                "start": date_article
            }
        }
    }
)

print("‚úÖ Article envoy√© dans Notion avec succ√®s !")

"""üìå Ce qu'on a accompli jusqu‚Äôici :
‚úÖ R√©cup√©ration d‚Äôun article depuis un vrai site d‚Äôactu IA
‚úÖ R√©sum√© ou reformulation automatique
‚úÖ G√©n√©ration d‚Äôun texte au format LinkedIn
‚úÖ Envoi automatique dans ta base Notion
"""

# üì¶ Imports
from notion_client import Client
from datetime import datetime

# üîê Ta cl√© API Notion (ne la partage √† personne)
notion = Client(auth="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")

# üÜî L‚ÄôID de ta base de donn√©es Notion (extrait de l‚ÄôURL)
database_id = "1bf902baf626806ea938db3de3c3be4d"

# üìù Exemple de contenu g√©n√©r√© automatiquement (remplace par ta variable `linkedin_post`)
linkedin_post = "Voici un exemple de texte LinkedIn g√©n√©r√© automatiquement √† partir d‚Äôun article IA."

# üì§ Envoi vers Notion
notion.pages.create(
    parent={"database_id": database_id},
    properties={
        "Name": {
            "title": [
                {
                    "text": {
                        "content": "Article g√©n√©r√© automatiquement"
                    }
                }
            ]
        },
        "Contenu": {
            "rich_text": [
                {
                    "text": {
                        "content": linkedin_post
                    }
                }
            ]
        },
        "Date": {
            "date": {
                "start": str(datetime.today())
            }
        }
    }
)

print("‚úÖ L‚Äôarticle a bien √©t√© envoy√© dans Notion !")

from notion_client import Client
from datetime import datetime

# Initialisation du client Notion avec ta cl√© API secr√®te
notion = Client(auth="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")

# ID de ta base de donn√©es Notion (extrait de l‚ÄôURL Notion)
database_id = "1bf902baf626806ea938db3de3c3be4d"

# Texte g√©n√©r√© (remplace ici par ton contenu LinkedIn r√©el)
linkedin_post = "Voici un exemple de texte LinkedIn g√©n√©r√© automatiquement √† partir d‚Äôun article IA."

# Envoi du contenu dans Notion
notion.pages.create(
    parent={"database_id": database_id},
    properties={
        "Name": {
            "title": [
                {
                    "text": {
                        "content": "Article g√©n√©r√© automatiquement"
                    }
                }
            ]
        },
        "Contenu": {
            "rich_text": [
                {
                    "text": {
                        "content": linkedin_post
                    }
                }
            ]
        },
        "Date": {
            "date": {
                "start": str(datetime.today().date())
            }
        }
    }
)

print("‚úÖ Article envoy√© dans Notion avec succ√®s

!pip install feedparser

import feedparser

# URL du flux RSS de Google News pour "intelligence artificielle" en fran√ßais
rss_url = "https://news.google.com/rss/search?q=intelligence+artificielle&hl=fr&gl=FR&ceid=FR:fr"

# R√©cup√©ration des articles
feed = feedparser.parse(rss_url)

# On limite √† 20 articles max
articles = feed.entries[:20]

# Affichage des titres et liens
for i, entry in enumerate(articles):
    print(f"üì∞ Article {i+1} : {entry.title}")
    print(f"üîó URL : {entry.link}\n")

import requests
from bs4 import BeautifulSoup

def extract_article_text(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        return text
    except:
        return ""

# Liste des articles exploitables
valid_articles = []

for i, entry in enumerate(articles):
    print(f"üîé Test de l'article {i+1} : {entry.title}")
    article_url = entry.link
    text = extract_article_text(article_url)

    if len(text) > 500:
        print(f"‚úÖ Article {i+1} exploitable avec {len(text)} caract√®res.")
        valid_articles.append((entry.title, article_url, text))
    else:
        print(f"‚ö†Ô∏è Article {i+1} trop court ({len(text)} caract√®res).\n")

print(f"\n‚úÖ {len(valid_articles)} articles exploitables trouv√©s.")

"""Yes üôà ! L√† on touche √† une limite tr√®s fr√©quente de Google News :
Les liens issus du flux RSS Google News ne m√®nent pas directement √† la source, mais √† une URL proxy de Google (https://news.google.com/rss/articles/...) ‚Üí ce lien redirige vers la vraie source dans le navigateur, mais pas dans un requests.get().

‚úÖ Solution fiable : r√©cup√©rer la vraie URL depuis le flux Google
Tu dois extraire l‚ÄôURL de redirection r√©elle dans chaque article du flux.

Voici comment corriger √ßa :

üîÅ Modifie la boucle pour obtenir la vraie URL de l‚Äôarticle
Remplace ta boucle par celle-ci :

‚úÖ Objectif :
Extraire automatiquement le contenu lisible de plusieurs articles √† partir des URLs trouv√©es via un flux Google News (ou autre source), en contournant les blocages comme les bandeaux de cookies.

üí° Strat√©gie avec Selenium :
Utilisation de Selenium + BeautifulSoup :
Tu charges la page avec Selenium pour contourner les cookies, le JS, etc.
Puis tu extrais le contenu via BeautifulSoup.
Ajout d‚Äôune fonction d'attente :
Pour attendre que le texte principal apparaisse, on utilise WebDriverWait.
Gestion des cookies :
Tu peux auto-accepter les cookies si tu identifies le bouton via un XPATH ou CSS_SELECTOR.
"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time

def extract_article_content(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)
        time.sleep(5)  # attendre le JS et potentiels cookies

        # Tenter de cliquer sur "Accepter les cookies"
        try:
            consent = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(., 'Accepter')]"))
            )
            consent.click()
            time.sleep(2)
        except:
            pass  # si pas de cookies, on continue

        # Attente de chargement du contenu
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, 'article'))  # ou 'p', 'main', selon la structure
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # Tu peux affiner ici la strat√©gie de scraping :
        article = soup.find('article')
        if not article:
            article = soup.find('main') or soup.find('body')

        text = article.get_text(separator="\n").strip()
        return text

    except Exception as e:
        print(f"Erreur sur l'URL {url} : {e}")
        return ""

    finally:
        driver.quit()

!pip install selenium
!apt-get update
!apt-get install -y chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0, '/usr/lib/chromium-browser/')

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

!pip install selenium

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

!pip install selenium
!apt-get update # mise √† jour des paquets
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0, '/usr/lib/chromium-browser/')

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')  # pas d'interface graphique
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)
    try:
        driver.get(url)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        text = soup.get_text()
        return text
    except Exception as e:
        print(f"Erreur lors du chargement de la page : {e}")
        return ""
    finally:
        driver.quit()

url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"
texte = get_text_with_selenium(url)
print(texte[:1000])  # pour voir les 1000 premiers caract√®res

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)

        # ‚úÖ Attente explicite de la DIV contenant le texte (ajuste si n√©cessaire)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # üîé Cibler plus pr√©cis√©ment le contenu de l‚Äôarticle
        article_tag = soup.select_one("article")

        if article_tag:
            return article_tag.get_text(separator="\n", strip=True)
        else:
            return "Aucun contenu trouv√© dans la balise <article>."

    except Exception as e:
        print(f"‚ùå Erreur sur l'URL {url} : {e}")
        return ""

    finally:
        driver.quit()

url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"
texte = get_text_with_selenium(url)
print(texte[:2000])  # Pour voir un aper√ßu plus large

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)

        # ‚è≥ Attendre que la div contenant le contenu soit charg√©e
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "article__content"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # üéØ R√©cup√©rer uniquement le contenu principal de l'article
        content_div = soup.find("div", class_="article__content")

        if content_div:
            # üßº Nettoyer le texte en retirant les boutons de partage, etc.
            for tag in content_div.select("button, .share, .author, .newsletter, script, style"):
                tag.decompose()

            text = content_div.get_text(separator="\n", strip=True)
            return text

        return "‚ùå Impossible de localiser le contenu principal."

    except Exception as e:
        return f"‚ùå Erreur lors de la r√©cup√©ration : {e}"

    finally:
        driver.quit()

url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"
texte = get_text_with_selenium(url)
print(texte[:2000])  # Un bon aper√ßu

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)

        # On essaie d'attendre un article (plus g√©n√©rique que "article__content")
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "article"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # On cherche l‚Äôarticle avec fallback si la classe pr√©cise n‚Äôexiste pas
        content = soup.find("div", class_="article__content")
        if not content:
            content = soup.find("article")

        if content:
            # On enl√®ve les √©l√©ments parasites
            for tag in content.select("button, .share, .author, .newsletter, script, style"):
                tag.decompose()

            return content.get_text(separator="\n", strip=True)

        return "‚ùå Aucun contenu principal trouv√©."

    except Exception as e:
        return f"‚ùå Erreur lors de la r√©cup√©ration : {e}"

    finally:
        driver.quit()

url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"
texte = get_text_with_selenium(url)
print(texte[:2000])  # Pour lire un gros aper√ßu

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time

def setup_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920x1080")
    return webdriver.Chrome(options=options)

def extract_links_from_google_news(query="intelligence artificielle", pages=2):
    driver = setup_driver()
    all_links = []

    for page in range(pages):
        start = page * 10
        search_url = f"https://www.google.com/search?q={query.replace(' ', '+')}&hl=fr&gl=FR&tbm=nws&start={start}"
        driver.get(search_url)
        time.sleep(3)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        results = soup.select('a')

        for a in results:
            href = a.get("href")
            if href and href.startswith("http") and "google.com" not in href:
                all_links.append(href)

    driver.quit()
    return list(set(all_links))  # pour √©viter les doublons

def get_article_text(url):
    driver = setup_driver()
    try:
        driver.get(url)
        time.sleep(5)

        # ‚ú® Contourner les cookies si pr√©sents
        try:
            cookie_buttons = driver.find_elements(By.XPATH, "//button[contains(text(), 'Accepter') or contains(text(), 'Tout accepter')]")
            if cookie_buttons:
                cookie_buttons[0].click()
                time.sleep(1)
        except:
            pass  # pas de cookies √† g√©rer

        # üîç Extraction du texte principal
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        paragraphs = soup.find_all(['p', 'h1', 'h2'])
        text = "\n".join([p.get_text() for p in paragraphs if p.get_text().strip()])
        return text

    except Exception as e:
        print(f"‚ùå Erreur pour l'URL : {url} ‚Äî {e}")
        return ""
    finally:
        driver.quit()

# üîÅ Pipeline complet
articles_links = extract_links_from_google_news()
print(f"üîó {len(articles_links)} liens r√©cup√©r√©s.\n")

for i, link in enumerate(articles_links):
    print(f"\nüìÑ Article {i+1} : {link}")
    texte = get_article_text(link)
    print(f"üìù Extrait : {texte[:500]}...")  # pour avoir un aper√ßu

"""üöÄ R√©sum√© du pipeline "Hackathon IA actu + Notion"
Scraping d‚Äôactualit√©s IA :
Utilise Selenium pour r√©cup√©rer le contenu des 2 premi√®res pages de r√©sultats Google News sur le th√®me ‚Äúintelligence artificielle‚Äù.
Le script contourne les bandeaux de cookies automatiquement.
Filtrage des articles exploitables :
Le code v√©rifie si le contenu est suffisant (nombre de caract√®res) pour √™tre trait√©.
Il ignore les pages qui ne sont pas des articles r√©els ou qui sont trop courtes.
R√©sum√© avec BART :
Utilise le mod√®le BART (Facebook) pour g√©n√©rer un r√©sum√© clair et concis de l‚Äôarticle r√©cup√©r√©.
G√©n√©ration d‚Äôun post LinkedIn :
Envoie le r√©sum√© √† l‚ÄôAPI OpenAI (gpt-3.5-turbo) pour g√©n√©rer un post LinkedIn professionnel.
Envoi vers Notion :
Le post est automatiquement ajout√© √† ta base de donn√©es ‚ÄúArticles IA‚Äù dans Notion, avec :
Le titre
Le post LinkedIn g√©n√©r√©
La date du jour

‚öôÔ∏è Technos utilis√©es
Selenium + BeautifulSoup pour extraire m√™me derri√®re des cookies
Transformers (BART) pour r√©sumer
OpenAI pour le contenu LinkedIn
Notion API pour tout consigner
Google News en source
"""

import torch

# V√©rifie si un GPU est dispo
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üîç Appareil utilis√© : {device.upper()}")

# üì¶ Installation des packages manquants
!pip install notion-client openai transformers selenium beautifulsoup4 --quiet
!apt-get update -qqy && apt-get install -qqy chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0, '/usr/lib/chromium-browser/')

"""code finale total"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from transformers import pipeline
from openai import OpenAI
from notion_client import Client
from datetime import datetime
import time
import re

# ‚ûï CONFIGURATION
NOTION_TOKEN = "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"
DATABASE_ID = "1bf902baf626806ea938db3de3c3be4d"
OPENAI_API_KEY = "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"

# ‚ûï INITIALISATION
notion = Client(auth=NOTION_TOKEN)
client = OpenAI(api_key=OPENAI_API_KEY)
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# ‚ûï SCRAPER SELENIUM
def get_text_with_selenium(url):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)
        time.sleep(5)

        # Accepter les cookies s'ils sont l√†
        try:
            bouton = WebDriverWait(driver, 3).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Accepter') or contains(text(), 'Tout accepter')]"))
            )
            bouton.click()
            time.sleep(2)
        except:
            pass  # Pas de bouton cookies

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        article_tags = soup.find_all("article")

        if not article_tags:
            article_tags = soup.find_all("div", class_=re.compile("content|main|article"))

        if article_tags:
            text = article_tags[0].get_text()
            return text
        else:
            raise ValueError("Aucun contenu principal trouv√©")

    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration : {e}")
        return ""
    finally:
        driver.quit()

# ‚ûï RESUME AVEC BART
def generate_summary(text):
    result = summarizer(text, max_length=200, min_length=60, do_sample=False)
    return result[0]['summary_text']

# ‚ûï GENERATION POST LINKEDIN
def generate_linkedin_post(summary):
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Tu es un expert en vulgarisation tech et IA. Fais un post LinkedIn engageant."},
            {"role": "user", "content": summary}
        ]
    )
    return response.choices[0].message.content

# ‚ûï ENVOI DANS NOTION
def send_to_notion(title, content):
    notion.pages.create(
        parent={"database_id": DATABASE_ID},
        properties={
            "Name": {
                "title": [
                    {"text": {"content": title}}
                ]
            },
            "Contenu": {
                "rich_text": [
                    {"text": {"content": content}}
                ]
            },
            "Date": {
                "date": {"start": str(datetime.today().date())}
            }
        }
    )

# ‚ûï EXTRACTION DES URL (Google News)
def get_google_news_urls():
    options = Options()
    options.add_argument("--headless")
    driver = webdriver.Chrome(options=options)
    driver.get("https://www.google.com/search?q=actualit%C3%A9s+intelligence+artificielle&hl=fr&gl=fr&tbm=nws")
    time.sleep(4)

    urls = set()
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    for link in soup.find_all('a'):
        href = link.get('href')
        if href and href.startswith("https") and "google" not in href:
            urls.add(href)

    # Deuxi√®me page
    try:
        next_btn = driver.find_element(By.ID, "pnnext")
        next_btn.click()
        time.sleep(4)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        for link in soup.find_all('a'):
            href = link.get('href')
            if href and href.startswith("https") and "google" not in href:
                urls.add(href)
    except:
        pass

    driver.quit()
    return list(urls)[:20]

# ‚ûï PIPELINE FINAL
def pipeline_actu():
    urls = get_google_news_urls()
    print(f"\n‚úÖ {len(urls)} URL collect√©es\n")

    for i, url in enumerate(urls, 1):
        print(f"\U0001F50D Traitement de l'article {i} : {url}")
        text

# üîß IMPORTS
import os
import sys
import time
import requests
import torch
from bs4 import BeautifulSoup
from datetime import datetime
from transformers import pipeline
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from notion_client import Client
from openai import OpenAI

# ‚úÖ CONFIGURATION
NOTION_TOKEN = "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"
DATABASE_ID = "1bf902baf626806ea938db3de3c3be4d"
OPENAI_API_KEY = "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"

notion = Client(auth=NOTION_TOKEN)
openai = OpenAI(api_key=OPENAI_API_KEY)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\n‚úÖ Appareil utilis√© : {device.upper()}")

summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=0 if device=="cuda" else -1)

# üîç SCRAPER GOOGLE NEWS

def get_google_news_urls():
    query = "intelligence artificielle"
    urls = set()
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)

    for page in range(0, 2):  # 2 pages de r√©sultats
        search_url = f"https://www.google.com/search?q={query}&hl=fr&gl=FR&ceid=FR:fr&tbm=nws&start={page*10}"
        driver.get(search_url)
        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a')))
        links = driver.find_elements(By.CSS_SELECTOR, 'a')
        for link in links:
            try:
                href = link.get_attribute('href')
                if href and href.startswith("https") and "google" not in href:
                    urls.add(href)
            except:
                pass

    driver.quit()
    return list(urls)[:20]

# üîé EXTRACT TEXT (avec contournement cookie)
def get_text_with_selenium(url):
    try:
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        driver = webdriver.Chrome(options=options)
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        driver.quit()

        paragraphs = soup.find_all(['p'])
        text = "\n".join(p.get_text() for p in paragraphs)
        return text.strip()

    except Exception as e:
        print(f"‚ùå Erreur sur {url} : {e}\n")
        return ""

# üß† OPENAI POUR POST LINKEDIN

def generate_linkedin_post(summary):
    prompt = f"√âcris un post LinkedIn professionnel, clair et accessible √† partir de ce r√©sum√© :\n\n{summary}"
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.7
    )
    return response.choices[0].message.content.strip()

# üßæ NOTION

def send_to_notion(title, content):
    notion.pages.create(
        parent={"database_id": DATABASE_ID},
        properties={
            "Name": {"title": [{"text": {"content": title}}]},
            "Date": {"date": {"start": str(datetime.today().date())}}
        },
        children=[{
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": [{"text": {"content": content}}]}
        }]
    )

# üöÄ PIPELINE FINAL

def pipeline_actu():
    urls = get_google_news_urls()
    print(f"\n‚úÖ {len(urls)} URL collect√©es\n")

    for i, url in enumerate(urls, 1):
        print(f"üîé Traitement de l'article {i} : {url}")
        texte = get_text_with_selenium(url)

        if len(texte) < 500:
            print("‚ö†Ô∏è Trop court. Skip.\n")
            continue

        try:
            summary = summarizer(texte, max_length=200, min_length=60, do_sample=False)[0]['summary_text']
            linkedin_post = generate_linkedin_post(summary)
            titre = texte.split("\n")[0][:50]
            send_to_notion(titre, linkedin_post)
            print("‚úÖ Ajout √† Notion r√©ussi !\n")
        except Exception as e:
            print(f"‚ùå Erreur sur {url} : {e}\n")

# ‚ñ∂Ô∏è EXECUTION
pipeline_actu()

# üîß IMPORTS
import os
import sys
import time
import requests
import torch
from bs4 import BeautifulSoup
from datetime import datetime
from transformers import pipeline
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from notion_client import Client
from openai import OpenAI

# ‚úÖ CONFIGURATION
NOTION_TOKEN = "ton_token_notion"
DATABASE_ID = "ton_id_bdd"
OPENAI_API_KEY = "ta_cl√©_openai"

notion = Client(auth=NOTION_TOKEN)
openai = OpenAI(api_key=OPENAI_API_KEY)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\n‚úÖ Appareil utilis√© : {device.upper()}")

summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=0 if device=="cuda" else -1)

# üîç SCRAPER GOOGLE NEWS

def get_google_news_urls():
    query = "intelligence artificielle"
    urls = set()
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)

    for page in range(0, 2):  # 2 pages de r√©sultats
        search_url = f"https://www.google.com/search?q={query}&hl=fr&gl=FR&ceid=FR:fr&tbm=nws&start={page*10}"
        driver.get(search_url)
        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a')))
        links = driver.find_elements(By.CSS_SELECTOR, 'a')
        for link in links:
            try:
                href = link.get_attribute('href')
                if href and href.startswith("https") and "google" not in href:
                    urls.add(href)
            except:
                pass

    driver.quit()
    return list(urls)[:20]

# üîé EXTRACT TEXT (avec contournement cookie)
def get_text_with_selenium(url):
    try:
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        driver = webdriver.Chrome(options=options)
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        driver.quit()

        paragraphs = soup.find_all(['p'])
        text = "\n".join(p.get_text() for p in paragraphs)
        return text.strip()

    except Exception as e:
        print(f"‚ùå Erreur sur {url} : {e}\n")
        return ""

# üß† OPENAI POUR POST LINKEDIN

def generate_linkedin_post(summary):
    prompt = f"√âcris un post LinkedIn professionnel, clair et accessible √† partir de ce r√©sum√© :\n\n{summary}"
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.7
    )
    return response.choices[0].message.content.strip()

# üßæ NOTION

def send_to_notion(title, content):
    notion.pages.create(
        parent={"database_id": DATABASE_ID},
        properties={
            "Name": {"title": [{"text": {"content": title}}]},
            "Date": {"date": {"start": str(datetime.today().date())}}
        },
        children=[{
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": [{"text": {"content": content}}]}
        }]
    )

# üöÄ PIPELINE FINAL

def pipeline_actu():
    urls = get_google_news_urls()
    print(f"\n‚úÖ {len(urls)} URL collect√©es\n")

    for i, url in enumerate(urls, 1):
        print(f"üîé Traitement de l'article {i} : {url}")
        texte = get_text_with_selenium(url)

        if len(texte) < 500:
            print("‚ö†Ô∏è Trop court. Skip.\n")
            continue

        try:
            summary = summarizer(texte, max_length=200, min_length=60, do_sample=False)[0]['summary_text']
            linkedin_post = generate_linkedin_post(summary)
            titre = texte.split("\n")[0][:50]
            send_to_notion(titre, linkedin_post)
            print("‚úÖ Ajout √† Notion r√©ussi !\n")
        except Exception as e:
            print(f"‚ùå Erreur sur {url} : {e}\n")

# ‚ñ∂Ô∏è EXECUTION
pipeline_actu()