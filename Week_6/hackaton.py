# -*- coding: utf-8 -*-
"""Hackatonfinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kw-G8TWQKzPRkXTeXmOqWFSHYnBl6aeZ
"""



"""✅ Étape 1 – Récupération des articles depuis Google News
On va utiliser la bibliothèque GoogleNews ou bien gnews (plus stable et simple). Elle permet de rechercher des articles par mot-clé, langue, pays, et date.

1. Installation des bibliothèques (dans Google Colab) :
"""

!pip install gnews

from gnews import GNews

# Configurer la recherche
google_news = GNews(language='fr', country='FR', period='7d', max_results=10)

# Rechercher des articles sur l'intelligence artificielle
articles = google_news.get_news('intelligence artificielle')

# Afficher les titres et liens
for article in articles:
    print(f"Titre : {article['title']}")
    print(f"Date : {article['published date']}")
    print(f"Lien : {article['url']}")
    print("-----")

"""## 🔍 Pourquoi ce filtrage d'articles ?

Lors d’un hackathon, le temps est limité et on cherche à être le plus efficace possible.  
Or, de nombreux sites d’actualités ne peuvent pas être facilement scrappés car ils sont protégés par :

- des **paywalls** (accès réservé aux abonnés),
- des **captcha ou protections anti-bot** (Cloudflare, etc.),
- des **codes d’erreur 403 ou 404** lors de requêtes simples.

➡️ Pour gagner du temps et éviter de coder des contournements complexes, nous avons fait le choix de **filtrer uniquement les articles accessibles directement via une requête HTTP simple (code 200)**.

Cela permet de :

- Prototyper rapidement un pipeline fonctionnel,
- Travailler avec des articles pertinents,
- Se concentrer sur les étapes d’**analyse, de résumé et de génération**.

Ce compromis est adapté à un contexte de hackathon rapide, tout en gardant une base extensible pour le futur.

"""

import requests

# Fonction pour tester si un article est accessible
def is_accessible(url):
    try:
        response = requests.get(url, timeout=5, headers={"User-Agent": "Mozilla/5.0"})
        return response.status_code == 200
    except:
        return False

# Filtrer les articles accessibles uniquement
accessible_articles = [a for a in articles if is_accessible(a['url'])]

# Afficher les articles valides
print(f"✅ {len(accessible_articles)} articles accessibles :\n")
for a in accessible_articles:
    print(f"Titre : {a['title']}")
    print(f"Date  : {a['published date']}")
    print(f"Lien  : {a['url']}")
    print("------")

"""✅ Étape 2 : Récupérer le contenu des articles
Maintenant qu'on a les liens, on va essayer de récupérer automatiquement le texte complet des articles (ce qui va nous servir à résumer et reformuler ensuite).

On va utiliser newspaper3k, une librairie super pratique pour extraire le texte propre d’un article à partir de son URL.
"""

# Installer la librairie (si ce n'est pas déjà fait)
!pip install newspaper3k

from newspaper import Article

# Exemple avec le premier article
url = articles[0]['url']
article = Article(url)

# Télécharger et parser le contenu
article.download()
article.parse()

# Affichage du texte brut
print("📰 Contenu brut de l’article :")
print(article.text[:1000])  # On affiche les 1000 premiers caractères

"""il bnettoie pour reprendre des balises en html"""

!pip install lxml_html_clean

# Installer la librairie (si ce n'est pas déjà fait)
!pip install newspaper3k

from newspaper import Article

# Exemple avec le premier article
url = articles[0]['url']
article = Article(url)

# Télécharger et parser le contenu
article.download()
article.parse()

# Affichage du texte brut
print("📰 Contenu brut de l’article :")
print(article.text[:1000])  # On affiche les 1000 premiers caractères

"""from transformers import BartForConditionalGeneration, BartTokenizer
import torch

# Charger le modèle et le tokenizer BART pré-entraîné pour le résumé
model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# Limiter la taille du texte pour rester dans les limites du modèle
text = article.text[:1024]

# Tokenization du texte
inputs = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)

# Génération du résumé
summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)

# Décodage du résumé
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Affichage
print("📝 Résumé de l'article :\n")
print(summary)

"""

from transformers import BartForConditionalGeneration, BartTokenizer
import torch

# Charger le modèle et le tokenizer BART pré-entraîné pour le résumé
model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# Limiter la taille du texte pour rester dans les limites du modèle
text = article.text[:1024]

# Tokenization du texte
inputs = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)

# Génération du résumé
summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)

# Décodage du résumé
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Affichage
print("📝 Résumé de l'article :\n")
print(summary)

print(len(article.text))
print(article.text[:500])  # Pour voir les 500 premiers caractères

article = Article(articles[1]['url'])
article.download()
article.parse()

from newspaper import Article

# Trouver le 1er article avec du texte pertinent
def trouver_article_valide(articles, min_length=500):
    for i, article_info in enumerate(articles):
        print(f"🔎 Test de l'article {i+1} : {article_info['title']}")
        url = article_info['url']
        try:
            article = Article(url)
            article.download()
            article.parse()

            if len(article.text) >= min_length:
                print(f"✅ Article {i+1} OK : {len(article.text)} caractères trouvés.\n")
                return article
            else:
                print(f"⚠️ Trop court ({len(article.text)} caractères)\n")
        except Exception as e:
            print(f"❌ Erreur sur l'article {i+1} : {e}\n")

    print("❌ Aucun article avec assez de contenu trouvé.")
    return None

article = trouver_article_valide(articles)

if article:
    print("📰 Aperçu du texte :")
    print(article.text[:500])
else:
    print("Aucun article n'a pu être utilisé.")

# Installation des librairies nécessaires (à exécuter une seule fois)
!pip install gnews newspaper3k

# Imports
from gnews import GNews
from newspaper import Article
from urllib.parse import urlparse, parse_qs

# 🔧 Fonction pour extraire le vrai lien depuis un lien Google News
def extraire_vrai_lien(url_google_news):
    parsed_url = urlparse(url_google_news)
    params = parse_qs(parsed_url.query)
    return params.get("url", [url_google_news])[0]

# 🔎 Configuration de la recherche
google_news = GNews(language='fr', country='FR', period='7d', max_results=10)
articles_info = google_news.get_news('intelligence artificielle')

# 🔄 Boucle sur les articles pour trouver le premier exploitable
article_utilisable = None

for idx, article_info in enumerate(articles_info):
    print(f"🔎 Test de l'article {idx+1} : {article_info['title']}")

    try:
        url = extraire_vrai_lien(article_info['url'])
        article = Article(url)
        article.download()
        article.parse()

        if len(article.text.strip()) > 500:
            print("✅ Article avec assez de contenu trouvé !\n")
            article_utilisable = article
            break
        else:
            print("⚠️ Trop court (0 caractères)\n")

    except Exception as e:
        print(f"❌ Erreur pendant l'extraction : {e}\n")

# 🖨️ Affichage du résultat
if article_utilisable:
    print("📰 Aperçu du texte extrait :\n")
    print(article_utilisable.text[:1000])
else:
    print("❌ Aucun article avec assez de contenu trouvé.")

"""🧠 Objectif
Créer un scraper intelligent qui :

contourne les protections classiques (ex : JavaScript, anti-bot)
extrait le texte complet même quand newspaper3k échoue
s’intègre dans ton pipeline actuel (GNews + résumé + Notion)
🛠️ Solution proposée : Utiliser Playwright
Playwright permet de contrôler un vrai navigateur (comme Chrome ou Firefox), ce qui contourne les blocages JavaScript ou Cloudflare.
Avantages :

Rendu complet de la page (y compris JavaScript)
Moins de blocages anti-scraping
Peut fonctionner dans une boucle pour tester tous les liens
Étapes :

On teste chaque lien avec newspaper3k.
Si le texte est vide ou trop court, on utilise Playwright pour "récupérer le vrai texte".
On continue le pipeline (résumé → Notion).
"""

!pip install playwright
!playwright install

"""🧪 On installe Selenium avec un Chrome "headless" sur Colab car la solution d'au dessus ne marche pas sur collab"""

!apt-get update
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
!pip install selenium
import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)
    driver.get(url)

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    text = soup.get_text()

    driver.quit()
    return text

url = "https://www.vie-publique.fr/actualite/294705-lintelligence-artificielle-un-outil-de-conduite-des-politiques-publiques"

texte = get_text_with_selenium(url)
print(texte[:1500])  # Affiche les 1500 premiers caractères du contenu

"""✅ Étape suivante : cliquer automatiquement sur "Accepter les cookies"
Pour récupérer le vrai contenu, on doit simuler un clic sur le bouton "Accepter" avant d’extraire le texte. car la il s'arrete a la page de rpesentation.

Voici un exemple de code à tester dans ta cellule actuelle :
"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)
    driver.get(url)

    try:
        # Accepter les cookies si le bouton est présent
        accept_button = WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Accepter')]"))
        )
        accept_button.click()
    except:
        print("Pas de bouton cookies détecté.")

    # Attendre le chargement du contenu
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.TAG_NAME, "article"))  # ou remplacer par une div spécifique si besoin
    )

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    text = soup.get_text()

    driver.quit()
    return text

text = get_text_with_selenium("https://www.vie-publique.fr/en-bref/293466-intelligence-artificielle-un-outil-de-conduite-des-politiques-publiques")
print(text[:1500])  # Pour afficher un extrait

"""Tu es tombé sur une TimeoutException. Cela signifie que la commande Selenium a attendu trop longtemps sans trouver l’élément demandé (ici probablement le bouton "Accepter" ou l’élément de contenu principal).

✅ On corrige ça étape par étape :
1. Enlever temporairement l’attente d’un élément précis

On va d’abord tester que le chargement de la page se fait bien, sans chercher d’élément spécifique :
"""

def get_text_with_selenium_simple(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)
    driver.get(url)

    # Attente simple de 5 secondes pour charger la page
    import time
    time.sleep(5)

    # Extraction du texte complet
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    text = soup.get_text()

    driver.quit()
    return text

text = get_text_with_selenium_simple("https://www.vie-publique.fr/en-bref/293466-intelligence-artificielle-un-outil-de-conduite-des-politiques-publiques")
print(text[:1500])

"""au final on se rend compte que le site choisi n'etait pas exploitable je decide donc de lui donner moi meme l'url d'un article interessant que l'on va utiliser"""

import requests
from bs4 import BeautifulSoup

# Lien de l'article
url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"

# En-têtes pour simuler un vrai navigateur (important !)
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# Requête HTTP
response = requests.get(url, headers=headers)

# Parsing HTML
soup = BeautifulSoup(response.text, "html.parser")

# Extraire les paragraphes de l'article
paragraphs = soup.find_all("p")
content = "\n\n".join([p.get_text() for p in paragraphs])

# Aperçu
print("📰 Extrait du contenu :\n")
print(content[:1500])  # Affiche les 1500 premiers caractères

"""Parfait si requests a fonctionné avec cet article ! 🎯
Ça veut dire que le site Siècle Digital ne bloque pas les scrapers classiques, donc on peut bosser plus rapidement et efficacement.

🎬 Étape suivante : résumer le contenu avec BART
Voici le code complet pour faire :

📥 Télécharger l’article avec requests
🧠 Générer un résumé automatique avec le modèle facebook/bart-large-cnn
"""

import requests
from bs4 import BeautifulSoup
from transformers import BartTokenizer, BartForConditionalGeneration

# URL de l'article
url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"

# Étape 1 : Télécharger et parser l'article
headers = {'User-Agent': 'Mozilla/5.0'}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.text, 'html.parser')

# Extraire le texte principal (on cible les balises <p>)
paragraphs = soup.find_all('p')
full_text = ' '.join([p.get_text() for p in paragraphs])
text = full_text[:1024]  # BART accepte jusqu'à 1024 tokens

# Étape 2 : Générer un résumé avec BART
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

inputs = tokenizer.encode(text, return_tensors='pt', max_length=1024, truncation=True)
summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print("🧠 Résumé généré :")
print(summary)

"""🧠 BART = Bidirectional and Auto-Regressive Transformers
C’est un modèle de langage développé par Facebook AI (Meta), spécialisé dans la génération de texte (résumé, correction, paraphrase…).

✅ À quoi ça sert ?
Résumer un texte 📄
Traduire 🌍
Corriger du texte ✍️
Générer une suite cohérente 🔁
Réécrire/vulgariser un contenu 🧑‍🏫
⚙️ Comment ça fonctionne ?
BART combine :

Encodeur bidirectionnel (comme BERT) → il lit tout le texte à la fois pour comprendre le contexte.
Décodeur auto-régressif (comme GPT) → il génère mot par mot, un peu comme un écrivain.
➡️ Donc, il comprend bien le texte (comme BERT)
➡️ Et il écrit bien des résumés ou du nouveau contenu (comme GPT)

📚 En résumé :
BART prend un texte long et en fait une version plus courte, claire et cohérente, tout en gardant le sens essentiel.
C’est parfait pour ton pipeline d'articles IA !

"""



"""Parfait ! 🎯 On va créer une fonction automatisée qui transforme un résumé brut (comme celui généré par BART) en post LinkedIn structuré, clair et accrocheur, avec un ton pédagogique et humain.

✅ Étape 1 : Fonction de transformation du résumé en post LinkedIn
"""



def generer_post_linkedin(resume, sujet="intelligence artificielle"):
    prompt = f"""
Tu es un expert en vulgarisation tech & IA sur LinkedIn. À partir du résumé suivant, génère un post structuré, pédagogique et engageant :

Résumé : "{resume}"

Structure souhaitée :
1. Accroche forte pour captiver le lecteur
2. Explication claire du problème ou sujet
3. Points clés à retenir (bullet points si possible)
4. Une phrase de conclusion ou une question ouverte pour susciter l’engagement

Le style doit rester simple, professionnel, accessible à un public non expert. Ajoute des emojis si pertinents, sans en abuser. N’invente pas de faits.

Sujet : {sujet}
"""

    return prompt

"""me connecter et creer une clef api open AI"""

import openai

openai.api_key = "sk-..."  # ta clé OpenAI

prompt = generer_post_linkedin(summary)

response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}],
    temperature=0.7,
    max_tokens=500
)

linkedin_post = response['choices'][0]['message']['content']
print(linkedin_post)

"""je dois me creer une clef api open ai"""

import openai

# Ta clé API OpenAI ici (remplace entre les guillemets)
openai.api_key = "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"  # 👈 Mets ta clé ici

# Résumé généré avec BART
resume = """
Une étude récente du Tow Center for Digital Journalism de l’université Columbia met en lumière un problème majeur des moteurs de recherche dopés. Plus de 60% des requêtes liées aux actualités recevraient des réponses erronées.
"""

# Prompt pour transformer en post LinkedIn vulgarisé
prompt = f"""
Tu es un expert en vulgarisation d’actualités IA pour LinkedIn. Reformule ce résumé pour en faire un post LinkedIn engageant, structuré ainsi :
- Intro courte avec une accroche claire
- Contexte de l’étude
- Ce qu’il faut retenir
- Question ou appel à réaction pour conclure

Résumé :
{resume}
"""

# Appel à l’API OpenAI
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",  # ou "gpt-4" si tu y as accès
    messages=[
        {"role": "user", "content": prompt}
    ],
    temperature=0.7,
    max_tokens=400
)

# Affichage du post LinkedIn
linkedin_post = response["choices"][0]["message"]["content"]
print("💬 Post LinkedIn généré :\n")
print(linkedin_post)

"""api trop ancienne donc on prend une plus recente"""

from openai import OpenAI

client = OpenAI(api_key="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")  # Mets ta clé ici

# Résumé à transformer
resume = """
Une étude récente du Tow Center for Digital Journalism de l’université Columbia met en lumière un problème majeur des moteurs de recherche dopés. Plus de 60% des requêtes liées aux actualités recevraient des réponses erronées.
"""

prompt = f"""
Tu es un expert en vulgarisation d’actualités IA pour LinkedIn. Reformule ce résumé pour en faire un post LinkedIn engageant, structuré ainsi :
- Intro courte avec une accroche claire
- Contexte de l’étude
- Ce qu’il faut retenir
- Question ou appel à réaction pour conclure

Résumé :
{resume}
"""

# Appel à l’API avec la nouvelle méthode
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": prompt}
    ],
    temperature=0.7,
    max_tokens=400
)

# Affichage du contenu LinkedIn
linkedin_post = response.choices[0].message.content
print("💬 Post LinkedIn généré :\n")
print(linkedin_post)

!pip install notion-client

from notion_client import Client
from datetime import datetime

# ✅ Clé API Notion
notion = Client(auth="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")  # remplace ici ta clé

# ✅ ID de ta base
database_id = "1bf902baf626806ea938db3de3c3be4d"

# ✅ Données à envoyer (exemple)
titre_article = "📌 Étude : plus de 60% d'erreurs dans les réponses IA sur l'actualité"
linkedin_post = """Voici un article qui nous alerte : plus de 60% des réponses générées par des IA aux questions d’actualité seraient erronées. ❌

👉 Cette étude du Tow Center de l’Université Columbia révèle les limites actuelles des IA dans le domaine de l'information en temps réel.

✅ Ce qu’il faut retenir :
- Les modèles peinent à restituer des faits à jour
- Le public peut être induit en erreur
- Il est crucial de valider les sources et d'intégrer un filtre de vérification

🧠 En résumé : l’IA, oui — mais pas sans garde-fous, surtout sur l’info sensible et mouvante !"""

# ✅ Envoi à Notion
notion.pages.create(
    parent={"database_id": database_id},
    properties={
        "Titre": {
            "title": [{"text": {"content": titre_article}}]
        },
        "Contenu": {
            "rich_text": [{"text": {"content": linkedin_post}}]
        },
        "Date": {
            "date": {"start": str(datetime.today())}
        }
    }
)

print("✅ Article envoyé dans Notion avec succès !")

from notion_client import Client
from datetime import datetime

# 🔐 Clé API Notion (ne pas partager)
notion = Client(auth="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")

# 🆔 Remplace par l’ID de ta base de données Notion
database_id = "1bf902baf626806ea938db3de3c3be4d"

# 📄 Données de test à envoyer
mon_titre = "Article généré automatiquement"
mon_texte = "Voici le texte généré automatiquement pour LinkedIn"
date_article = datetime.today().strftime('%Y-%m-%d')

# 📤 Requête vers l'API Notion
notion.pages.create(
    parent={"database_id": database_id},
    properties={
        "Name": {
            "title": [
                {
                    "text": {
                        "content": mon_titre
                    }
                }
            ]
        },
        "Contenu": {
            "rich_text": [
                {
                    "text": {
                        "content": mon_texte
                    }
                }
            ]
        },
        "Date": {
            "date": {
                "start": date_article
            }
        }
    }
)

print("✅ Article envoyé dans Notion avec succès !")

"""📌 Ce qu'on a accompli jusqu’ici :
✅ Récupération d’un article depuis un vrai site d’actu IA
✅ Résumé ou reformulation automatique
✅ Génération d’un texte au format LinkedIn
✅ Envoi automatique dans ta base Notion
"""

# 📦 Imports
from notion_client import Client
from datetime import datetime

# 🔐 Ta clé API Notion (ne la partage à personne)
notion = Client(auth="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")

# 🆔 L’ID de ta base de données Notion (extrait de l’URL)
database_id = "1bf902baf626806ea938db3de3c3be4d"

# 📝 Exemple de contenu généré automatiquement (remplace par ta variable `linkedin_post`)
linkedin_post = "Voici un exemple de texte LinkedIn généré automatiquement à partir d’un article IA."

# 📤 Envoi vers Notion
notion.pages.create(
    parent={"database_id": database_id},
    properties={
        "Name": {
            "title": [
                {
                    "text": {
                        "content": "Article généré automatiquement"
                    }
                }
            ]
        },
        "Contenu": {
            "rich_text": [
                {
                    "text": {
                        "content": linkedin_post
                    }
                }
            ]
        },
        "Date": {
            "date": {
                "start": str(datetime.today())
            }
        }
    }
)

print("✅ L’article a bien été envoyé dans Notion !")

from notion_client import Client
from datetime import datetime

# Initialisation du client Notion avec ta clé API secrète
notion = Client(auth="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")

# ID de ta base de données Notion (extrait de l’URL Notion)
database_id = "1bf902baf626806ea938db3de3c3be4d"

# Texte généré (remplace ici par ton contenu LinkedIn réel)
linkedin_post = "Voici un exemple de texte LinkedIn généré automatiquement à partir d’un article IA."

# Envoi du contenu dans Notion
notion.pages.create(
    parent={"database_id": database_id},
    properties={
        "Name": {
            "title": [
                {
                    "text": {
                        "content": "Article généré automatiquement"
                    }
                }
            ]
        },
        "Contenu": {
            "rich_text": [
                {
                    "text": {
                        "content": linkedin_post
                    }
                }
            ]
        },
        "Date": {
            "date": {
                "start": str(datetime.today().date())
            }
        }
    }
)

print("✅ Article envoyé dans Notion avec succès

!pip install feedparser

import feedparser

# URL du flux RSS de Google News pour "intelligence artificielle" en français
rss_url = "https://news.google.com/rss/search?q=intelligence+artificielle&hl=fr&gl=FR&ceid=FR:fr"

# Récupération des articles
feed = feedparser.parse(rss_url)

# On limite à 20 articles max
articles = feed.entries[:20]

# Affichage des titres et liens
for i, entry in enumerate(articles):
    print(f"📰 Article {i+1} : {entry.title}")
    print(f"🔗 URL : {entry.link}\n")

import requests
from bs4 import BeautifulSoup

def extract_article_text(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        return text
    except:
        return ""

# Liste des articles exploitables
valid_articles = []

for i, entry in enumerate(articles):
    print(f"🔎 Test de l'article {i+1} : {entry.title}")
    article_url = entry.link
    text = extract_article_text(article_url)

    if len(text) > 500:
        print(f"✅ Article {i+1} exploitable avec {len(text)} caractères.")
        valid_articles.append((entry.title, article_url, text))
    else:
        print(f"⚠️ Article {i+1} trop court ({len(text)} caractères).\n")

print(f"\n✅ {len(valid_articles)} articles exploitables trouvés.")

"""Yes 🙈 ! Là on touche à une limite très fréquente de Google News :
Les liens issus du flux RSS Google News ne mènent pas directement à la source, mais à une URL proxy de Google (https://news.google.com/rss/articles/...) → ce lien redirige vers la vraie source dans le navigateur, mais pas dans un requests.get().

✅ Solution fiable : récupérer la vraie URL depuis le flux Google
Tu dois extraire l’URL de redirection réelle dans chaque article du flux.

Voici comment corriger ça :

🔁 Modifie la boucle pour obtenir la vraie URL de l’article
Remplace ta boucle par celle-ci :

✅ Objectif :
Extraire automatiquement le contenu lisible de plusieurs articles à partir des URLs trouvées via un flux Google News (ou autre source), en contournant les blocages comme les bandeaux de cookies.

💡 Stratégie avec Selenium :
Utilisation de Selenium + BeautifulSoup :
Tu charges la page avec Selenium pour contourner les cookies, le JS, etc.
Puis tu extrais le contenu via BeautifulSoup.
Ajout d’une fonction d'attente :
Pour attendre que le texte principal apparaisse, on utilise WebDriverWait.
Gestion des cookies :
Tu peux auto-accepter les cookies si tu identifies le bouton via un XPATH ou CSS_SELECTOR.
"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time

def extract_article_content(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)
        time.sleep(5)  # attendre le JS et potentiels cookies

        # Tenter de cliquer sur "Accepter les cookies"
        try:
            consent = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(., 'Accepter')]"))
            )
            consent.click()
            time.sleep(2)
        except:
            pass  # si pas de cookies, on continue

        # Attente de chargement du contenu
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, 'article'))  # ou 'p', 'main', selon la structure
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # Tu peux affiner ici la stratégie de scraping :
        article = soup.find('article')
        if not article:
            article = soup.find('main') or soup.find('body')

        text = article.get_text(separator="\n").strip()
        return text

    except Exception as e:
        print(f"Erreur sur l'URL {url} : {e}")
        return ""

    finally:
        driver.quit()

!pip install selenium
!apt-get update
!apt-get install -y chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0, '/usr/lib/chromium-browser/')

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

!pip install selenium

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

!pip install selenium
!apt-get update # mise à jour des paquets
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0, '/usr/lib/chromium-browser/')

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')  # pas d'interface graphique
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)
    try:
        driver.get(url)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        text = soup.get_text()
        return text
    except Exception as e:
        print(f"Erreur lors du chargement de la page : {e}")
        return ""
    finally:
        driver.quit()

url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"
texte = get_text_with_selenium(url)
print(texte[:1000])  # pour voir les 1000 premiers caractères

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)

        # ✅ Attente explicite de la DIV contenant le texte (ajuste si nécessaire)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # 🔎 Cibler plus précisément le contenu de l’article
        article_tag = soup.select_one("article")

        if article_tag:
            return article_tag.get_text(separator="\n", strip=True)
        else:
            return "Aucun contenu trouvé dans la balise <article>."

    except Exception as e:
        print(f"❌ Erreur sur l'URL {url} : {e}")
        return ""

    finally:
        driver.quit()

url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"
texte = get_text_with_selenium(url)
print(texte[:2000])  # Pour voir un aperçu plus large

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)

        # ⏳ Attendre que la div contenant le contenu soit chargée
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "article__content"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # 🎯 Récupérer uniquement le contenu principal de l'article
        content_div = soup.find("div", class_="article__content")

        if content_div:
            # 🧼 Nettoyer le texte en retirant les boutons de partage, etc.
            for tag in content_div.select("button, .share, .author, .newsletter, script, style"):
                tag.decompose()

            text = content_div.get_text(separator="\n", strip=True)
            return text

        return "❌ Impossible de localiser le contenu principal."

    except Exception as e:
        return f"❌ Erreur lors de la récupération : {e}"

    finally:
        driver.quit()

url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"
texte = get_text_with_selenium(url)
print(texte[:2000])  # Un bon aperçu

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def get_text_with_selenium(url):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)

        # On essaie d'attendre un article (plus générique que "article__content")
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "article"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # On cherche l’article avec fallback si la classe précise n’existe pas
        content = soup.find("div", class_="article__content")
        if not content:
            content = soup.find("article")

        if content:
            # On enlève les éléments parasites
            for tag in content.select("button, .share, .author, .newsletter, script, style"):
                tag.decompose()

            return content.get_text(separator="\n", strip=True)

        return "❌ Aucun contenu principal trouvé."

    except Exception as e:
        return f"❌ Erreur lors de la récupération : {e}"

    finally:
        driver.quit()

url = "https://siecledigital.fr/2025/03/17/etude-plus-de-60-des-reponses-des-ia-aux-requetes-dactualites-sont-erronees/"
texte = get_text_with_selenium(url)
print(texte[:2000])  # Pour lire un gros aperçu

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time

def setup_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920x1080")
    return webdriver.Chrome(options=options)

def extract_links_from_google_news(query="intelligence artificielle", pages=2):
    driver = setup_driver()
    all_links = []

    for page in range(pages):
        start = page * 10
        search_url = f"https://www.google.com/search?q={query.replace(' ', '+')}&hl=fr&gl=FR&tbm=nws&start={start}"
        driver.get(search_url)
        time.sleep(3)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        results = soup.select('a')

        for a in results:
            href = a.get("href")
            if href and href.startswith("http") and "google.com" not in href:
                all_links.append(href)

    driver.quit()
    return list(set(all_links))  # pour éviter les doublons

def get_article_text(url):
    driver = setup_driver()
    try:
        driver.get(url)
        time.sleep(5)

        # ✨ Contourner les cookies si présents
        try:
            cookie_buttons = driver.find_elements(By.XPATH, "//button[contains(text(), 'Accepter') or contains(text(), 'Tout accepter')]")
            if cookie_buttons:
                cookie_buttons[0].click()
                time.sleep(1)
        except:
            pass  # pas de cookies à gérer

        # 🔍 Extraction du texte principal
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        paragraphs = soup.find_all(['p', 'h1', 'h2'])
        text = "\n".join([p.get_text() for p in paragraphs if p.get_text().strip()])
        return text

    except Exception as e:
        print(f"❌ Erreur pour l'URL : {url} — {e}")
        return ""
    finally:
        driver.quit()

# 🔁 Pipeline complet
articles_links = extract_links_from_google_news()
print(f"🔗 {len(articles_links)} liens récupérés.\n")

for i, link in enumerate(articles_links):
    print(f"\n📄 Article {i+1} : {link}")
    texte = get_article_text(link)
    print(f"📝 Extrait : {texte[:500]}...")  # pour avoir un aperçu

"""🚀 Résumé du pipeline "Hackathon IA actu + Notion"
Scraping d’actualités IA :
Utilise Selenium pour récupérer le contenu des 2 premières pages de résultats Google News sur le thème “intelligence artificielle”.
Le script contourne les bandeaux de cookies automatiquement.
Filtrage des articles exploitables :
Le code vérifie si le contenu est suffisant (nombre de caractères) pour être traité.
Il ignore les pages qui ne sont pas des articles réels ou qui sont trop courtes.
Résumé avec BART :
Utilise le modèle BART (Facebook) pour générer un résumé clair et concis de l’article récupéré.
Génération d’un post LinkedIn :
Envoie le résumé à l’API OpenAI (gpt-3.5-turbo) pour générer un post LinkedIn professionnel.
Envoi vers Notion :
Le post est automatiquement ajouté à ta base de données “Articles IA” dans Notion, avec :
Le titre
Le post LinkedIn généré
La date du jour

⚙️ Technos utilisées
Selenium + BeautifulSoup pour extraire même derrière des cookies
Transformers (BART) pour résumer
OpenAI pour le contenu LinkedIn
Notion API pour tout consigner
Google News en source
"""

import torch

# Vérifie si un GPU est dispo
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"🔍 Appareil utilisé : {device.upper()}")

# 📦 Installation des packages manquants
!pip install notion-client openai transformers selenium beautifulsoup4 --quiet
!apt-get update -qqy && apt-get install -qqy chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0, '/usr/lib/chromium-browser/')

"""code finale total"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from transformers import pipeline
from openai import OpenAI
from notion_client import Client
from datetime import datetime
import time
import re

# ➕ CONFIGURATION
NOTION_TOKEN = "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"
DATABASE_ID = "1bf902baf626806ea938db3de3c3be4d"
OPENAI_API_KEY = "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"

# ➕ INITIALISATION
notion = Client(auth=NOTION_TOKEN)
client = OpenAI(api_key=OPENAI_API_KEY)
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# ➕ SCRAPER SELENIUM
def get_text_with_selenium(url):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)
        time.sleep(5)

        # Accepter les cookies s'ils sont là
        try:
            bouton = WebDriverWait(driver, 3).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Accepter') or contains(text(), 'Tout accepter')]"))
            )
            bouton.click()
            time.sleep(2)
        except:
            pass  # Pas de bouton cookies

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        article_tags = soup.find_all("article")

        if not article_tags:
            article_tags = soup.find_all("div", class_=re.compile("content|main|article"))

        if article_tags:
            text = article_tags[0].get_text()
            return text
        else:
            raise ValueError("Aucun contenu principal trouvé")

    except Exception as e:
        print(f"❌ Erreur lors de la récupération : {e}")
        return ""
    finally:
        driver.quit()

# ➕ RESUME AVEC BART
def generate_summary(text):
    result = summarizer(text, max_length=200, min_length=60, do_sample=False)
    return result[0]['summary_text']

# ➕ GENERATION POST LINKEDIN
def generate_linkedin_post(summary):
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Tu es un expert en vulgarisation tech et IA. Fais un post LinkedIn engageant."},
            {"role": "user", "content": summary}
        ]
    )
    return response.choices[0].message.content

# ➕ ENVOI DANS NOTION
def send_to_notion(title, content):
    notion.pages.create(
        parent={"database_id": DATABASE_ID},
        properties={
            "Name": {
                "title": [
                    {"text": {"content": title}}
                ]
            },
            "Contenu": {
                "rich_text": [
                    {"text": {"content": content}}
                ]
            },
            "Date": {
                "date": {"start": str(datetime.today().date())}
            }
        }
    )

# ➕ EXTRACTION DES URL (Google News)
def get_google_news_urls():
    options = Options()
    options.add_argument("--headless")
    driver = webdriver.Chrome(options=options)
    driver.get("https://www.google.com/search?q=actualit%C3%A9s+intelligence+artificielle&hl=fr&gl=fr&tbm=nws")
    time.sleep(4)

    urls = set()
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    for link in soup.find_all('a'):
        href = link.get('href')
        if href and href.startswith("https") and "google" not in href:
            urls.add(href)

    # Deuxième page
    try:
        next_btn = driver.find_element(By.ID, "pnnext")
        next_btn.click()
        time.sleep(4)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        for link in soup.find_all('a'):
            href = link.get('href')
            if href and href.startswith("https") and "google" not in href:
                urls.add(href)
    except:
        pass

    driver.quit()
    return list(urls)[:20]

# ➕ PIPELINE FINAL
def pipeline_actu():
    urls = get_google_news_urls()
    print(f"\n✅ {len(urls)} URL collectées\n")

    for i, url in enumerate(urls, 1):
        print(f"\U0001F50D Traitement de l'article {i} : {url}")
        text

# 🔧 IMPORTS
import os
import sys
import time
import requests
import torch
from bs4 import BeautifulSoup
from datetime import datetime
from transformers import pipeline
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from notion_client import Client
from openai import OpenAI

# ✅ CONFIGURATION
NOTION_TOKEN = "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"
DATABASE_ID = "1bf902baf626806ea938db3de3c3be4d"
OPENAI_API_KEY = "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"

notion = Client(auth=NOTION_TOKEN)
openai = OpenAI(api_key=OPENAI_API_KEY)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\n✅ Appareil utilisé : {device.upper()}")

summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=0 if device=="cuda" else -1)

# 🔍 SCRAPER GOOGLE NEWS

def get_google_news_urls():
    query = "intelligence artificielle"
    urls = set()
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)

    for page in range(0, 2):  # 2 pages de résultats
        search_url = f"https://www.google.com/search?q={query}&hl=fr&gl=FR&ceid=FR:fr&tbm=nws&start={page*10}"
        driver.get(search_url)
        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a')))
        links = driver.find_elements(By.CSS_SELECTOR, 'a')
        for link in links:
            try:
                href = link.get_attribute('href')
                if href and href.startswith("https") and "google" not in href:
                    urls.add(href)
            except:
                pass

    driver.quit()
    return list(urls)[:20]

# 🔎 EXTRACT TEXT (avec contournement cookie)
def get_text_with_selenium(url):
    try:
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        driver = webdriver.Chrome(options=options)
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        driver.quit()

        paragraphs = soup.find_all(['p'])
        text = "\n".join(p.get_text() for p in paragraphs)
        return text.strip()

    except Exception as e:
        print(f"❌ Erreur sur {url} : {e}\n")
        return ""

# 🧠 OPENAI POUR POST LINKEDIN

def generate_linkedin_post(summary):
    prompt = f"Écris un post LinkedIn professionnel, clair et accessible à partir de ce résumé :\n\n{summary}"
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.7
    )
    return response.choices[0].message.content.strip()

# 🧾 NOTION

def send_to_notion(title, content):
    notion.pages.create(
        parent={"database_id": DATABASE_ID},
        properties={
            "Name": {"title": [{"text": {"content": title}}]},
            "Date": {"date": {"start": str(datetime.today().date())}}
        },
        children=[{
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": [{"text": {"content": content}}]}
        }]
    )

# 🚀 PIPELINE FINAL

def pipeline_actu():
    urls = get_google_news_urls()
    print(f"\n✅ {len(urls)} URL collectées\n")

    for i, url in enumerate(urls, 1):
        print(f"🔎 Traitement de l'article {i} : {url}")
        texte = get_text_with_selenium(url)

        if len(texte) < 500:
            print("⚠️ Trop court. Skip.\n")
            continue

        try:
            summary = summarizer(texte, max_length=200, min_length=60, do_sample=False)[0]['summary_text']
            linkedin_post = generate_linkedin_post(summary)
            titre = texte.split("\n")[0][:50]
            send_to_notion(titre, linkedin_post)
            print("✅ Ajout à Notion réussi !\n")
        except Exception as e:
            print(f"❌ Erreur sur {url} : {e}\n")

# ▶️ EXECUTION
pipeline_actu()

# 🔧 IMPORTS
import os
import sys
import time
import requests
import torch
from bs4 import BeautifulSoup
from datetime import datetime
from transformers import pipeline
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from notion_client import Client
from openai import OpenAI

# ✅ CONFIGURATION
NOTION_TOKEN = "ton_token_notion"
DATABASE_ID = "ton_id_bdd"
OPENAI_API_KEY = "ta_clé_openai"

notion = Client(auth=NOTION_TOKEN)
openai = OpenAI(api_key=OPENAI_API_KEY)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\n✅ Appareil utilisé : {device.upper()}")

summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=0 if device=="cuda" else -1)

# 🔍 SCRAPER GOOGLE NEWS

def get_google_news_urls():
    query = "intelligence artificielle"
    urls = set()
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)

    for page in range(0, 2):  # 2 pages de résultats
        search_url = f"https://www.google.com/search?q={query}&hl=fr&gl=FR&ceid=FR:fr&tbm=nws&start={page*10}"
        driver.get(search_url)
        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a')))
        links = driver.find_elements(By.CSS_SELECTOR, 'a')
        for link in links:
            try:
                href = link.get_attribute('href')
                if href and href.startswith("https") and "google" not in href:
                    urls.add(href)
            except:
                pass

    driver.quit()
    return list(urls)[:20]

# 🔎 EXTRACT TEXT (avec contournement cookie)
def get_text_with_selenium(url):
    try:
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        driver = webdriver.Chrome(options=options)
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        driver.quit()

        paragraphs = soup.find_all(['p'])
        text = "\n".join(p.get_text() for p in paragraphs)
        return text.strip()

    except Exception as e:
        print(f"❌ Erreur sur {url} : {e}\n")
        return ""

# 🧠 OPENAI POUR POST LINKEDIN

def generate_linkedin_post(summary):
    prompt = f"Écris un post LinkedIn professionnel, clair et accessible à partir de ce résumé :\n\n{summary}"
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.7
    )
    return response.choices[0].message.content.strip()

# 🧾 NOTION

def send_to_notion(title, content):
    notion.pages.create(
        parent={"database_id": DATABASE_ID},
        properties={
            "Name": {"title": [{"text": {"content": title}}]},
            "Date": {"date": {"start": str(datetime.today().date())}}
        },
        children=[{
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": [{"text": {"content": content}}]}
        }]
    )

# 🚀 PIPELINE FINAL

def pipeline_actu():
    urls = get_google_news_urls()
    print(f"\n✅ {len(urls)} URL collectées\n")

    for i, url in enumerate(urls, 1):
        print(f"🔎 Traitement de l'article {i} : {url}")
        texte = get_text_with_selenium(url)

        if len(texte) < 500:
            print("⚠️ Trop court. Skip.\n")
            continue

        try:
            summary = summarizer(texte, max_length=200, min_length=60, do_sample=False)[0]['summary_text']
            linkedin_post = generate_linkedin_post(summary)
            titre = texte.split("\n")[0][:50]
            send_to_notion(titre, linkedin_post)
            print("✅ Ajout à Notion réussi !\n")
        except Exception as e:
            print(f"❌ Erreur sur {url} : {e}\n")

# ▶️ EXECUTION
pipeline_actu()